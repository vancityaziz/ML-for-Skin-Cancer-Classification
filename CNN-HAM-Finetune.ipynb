{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63acada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sabad\\OneDrive\\Desktop\\APS360\\HAM10K Dataset\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import timm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4514d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"C:/Users/sabad/OneDrive/Desktop/APS360/HAM10K Dataset/HAM10K/HAM10000_metadata.csv\"\n",
    "\n",
    "# Load metadata\n",
    "df = pd.read_csv(metadata_path)\n",
    "\n",
    "# Drop samples with missing values in key metadata fields\n",
    "df = df.dropna(subset=['age', 'sex', 'localization'])\n",
    "\n",
    "# Normalize age\n",
    "scaler = MinMaxScaler()\n",
    "df['age_norm'] = scaler.fit_transform(df[['age']])\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "df = pd.get_dummies(df, columns=['sex', 'localization'], drop_first=False)\n",
    "\n",
    "# Define metadata feature columns\n",
    "meta_features = ['age_norm'] + [col for col in df.columns if col.startswith('sex_') or col.startswith('localization_')]\n",
    "df['meta'] = df[meta_features].values.tolist()\n",
    "\n",
    "# Encode labels\n",
    "label_map = {label: i for i, label in enumerate(df['dx'].unique())}\n",
    "df['label'] = df['dx'].map(label_map)\n",
    "\n",
    "img_dirs = [\n",
    "    \"C:/Users/sabad/OneDrive/Desktop/APS360/HAM10K Dataset/HAM10K/HAM10000_images_part_1\",\n",
    "    \"C:/Users/sabad/OneDrive/Desktop/APS360/HAM10K Dataset/HAM10K/HAM10000_images_part_2\"\n",
    "]\n",
    "\n",
    "def find_image_path(image_id):\n",
    "    for dir_path in img_dirs:\n",
    "        path = os.path.join(dir_path, f\"{image_id}.jpg\")\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "df['image_path'] = df['image_id'].apply(find_image_path)\n",
    "\n",
    "unique_ids = df['lesion_id'].unique()\n",
    "train_ids, temp_ids = train_test_split(unique_ids, test_size=0.3, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df = df[df['lesion_id'].isin(train_ids)]\n",
    "val_df = df[df['lesion_id'].isin(val_ids)]\n",
    "test_df = df[df['lesion_id'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9382d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class HAM10000FusionDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = Image.open(row['image_path']).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load metadata\n",
    "        meta = torch.tensor(row['meta'], dtype=torch.float32)\n",
    "\n",
    "        # Load label\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "\n",
    "        return image, meta, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a78d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dirs, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dirs = img_dirs  # List of directories\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_name = row['image_id'] + \".jpg\"\n",
    "\n",
    "        # Look for the image in both directories\n",
    "        for dir_path in self.img_dirs:\n",
    "            image_path = os.path.join(dir_path, image_name)\n",
    "            if os.path.exists(image_path):\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{image_name} not found in provided directories.\")\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = row['label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdd16988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age_norm', 'sex_female', 'sex_male', 'sex_unknown', 'localization_abdomen', 'localization_acral', 'localization_back', 'localization_chest', 'localization_ear', 'localization_face', 'localization_foot', 'localization_genital', 'localization_hand', 'localization_lower extremity', 'localization_neck', 'localization_scalp', 'localization_trunk', 'localization_unknown', 'localization_upper extremity']\n",
      "Metadata input dimension: 19\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# train_dataset = HAM10000Dataset(train_df, img_dirs, transform=train_transform)\n",
    "# val_dataset = HAM10000Dataset(val_df, img_dirs, transform=val_transform)\n",
    "# test_dataset = HAM10000Dataset(test_df, img_dirs, transform=val_transform)\n",
    "\n",
    "train_dataset = HAM10000FusionDataset(train_df, transform=train_transform)\n",
    "val_dataset = HAM10000FusionDataset(val_df, transform=val_transform)\n",
    "test_dataset = HAM10000FusionDataset(test_df, transform=val_transform)\n",
    "\n",
    "label_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = 1. / label_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)  # Normalize\n",
    "\n",
    "label_to_weight = {label: class_weights[i] for i, label in enumerate(sorted(train_df['label'].unique()))}\n",
    "sample_weights = train_df['label'].map(label_to_weight).values\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    sampler=sampler\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "meta_input_dim = len(meta_features)\n",
    "print(meta_features)\n",
    "print(\"Metadata input dimension:\", meta_input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "154e69e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 992 samples\n",
      "Class 1: 1038 samples\n",
      "Class 2: 946 samples\n",
      "Class 3: 1016 samples\n",
      "Class 4: 958 samples\n",
      "Class 5: 1018 samples\n",
      "Class 6: 989 samples\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count how many samples of each class appear after sampling\n",
    "label_counter = Counter()\n",
    "\n",
    "for images, meta, labels in train_loader:\n",
    "    labels = labels.cpu().numpy()\n",
    "    label_counter.update(labels)\n",
    "\n",
    "# Print counts\n",
    "for label, count in sorted(label_counter.items()):\n",
    "    print(f\"Class {label}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c3d81",
   "metadata": {},
   "source": [
    "**ResNet50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faad7ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet50_model(num_classes):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd3b96",
   "metadata": {},
   "source": [
    "**EfficientNet B0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328b8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetFinetune(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.backbone = models.efficientnet_b2(weights='EfficientNet_B2_Weights.DEFAULT')\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "\n",
    "        # Replace classifier\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f5b41",
   "metadata": {},
   "source": [
    "**Fusion (CNN + MLP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f347b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, image_model_name='efficientnet_b0', meta_input_dim=19, num_classes=7):\n",
    "        super(FusionModel, self).__init__()\n",
    "\n",
    "        # Image branch (pretrained EfficientNet)\n",
    "        self.image_model = models.efficientnet_b0(weights='EfficientNet_B0_Weights.DEFAULT')\n",
    "        in_features = self.image_model.classifier[1].in_features\n",
    "        self.image_model.classifier = nn.Identity()  # Remove the original classifier\n",
    "\n",
    "        # Metadata branch (MLP)\n",
    "        self.meta_net = nn.Sequential(\n",
    "            nn.Linear(meta_input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        # Fusion classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features + 32, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, metadata):\n",
    "        img_feat = self.image_model(image)           # [B, in_features]\n",
    "        meta_feat = self.meta_net(metadata.float())  # [B, 32]\n",
    "        x = torch.cat([img_feat, meta_feat], dim=1)  # [B, in_features + 32]\n",
    "        out = self.classifier(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1a3fb",
   "metadata": {},
   "source": [
    "**Generic Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "520fc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = model.state_dict()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # ---------- Training ----------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        # ---------- Validation ----------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "            print(\"Best model updated\")\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5a183",
   "metadata": {},
   "source": [
    "**Fusion Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51dd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_fusion_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # ---------- TRAIN ----------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        for images, metadata, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            images = images.to(device)\n",
    "            metadata = metadata.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, metadata)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        # ---------- VALIDATION ----------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, metadata, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                images = images.to(device)\n",
    "                metadata = metadata.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images, metadata)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        # ---------- Log ----------\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # ---------- Save best model ----------\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            print(\"✅ Best model updated!\")\n",
    "\n",
    "    # Load best weights before returning\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480099e",
   "metadata": {},
   "source": [
    "**Two Phase Fusion Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0596d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all_but_meta_and_classifier(model):\n",
    "    # Freeze image model\n",
    "    for param in model.image_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze meta_net and classifier\n",
    "    for param in model.meta_net.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "def unfreeze_last_block(model):\n",
    "    for param in model.image_model.features[-1].parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5ee8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_phase_fusion(model, train_loader, val_loader, device, phase1_epochs=5, phase2_epochs=5, wd=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # -------- Phase 1: Freeze CNN, train only MLP + classifier --------\n",
    "    print(\"\\nPhase 1: Training metadata branch and fusion classifier\")\n",
    "    freeze_all_but_meta_and_classifier(model)\n",
    "    optimizer1 = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, weight_decay=wd)\n",
    "    model = train_fusion_model(model, train_loader, val_loader, criterion, optimizer1, device, num_epochs=phase1_epochs)\n",
    "\n",
    "    # -------- Phase 2: Unfreeze last CNN block --------\n",
    "    print(\"\\nPhase 2: Fine-tune last EfficientNet block with rest of model\")\n",
    "    unfreeze_last_block(model)\n",
    "    optimizer2 = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5, weight_decay=wd)\n",
    "    model = train_fusion_model(model, train_loader, val_loader, criterion, optimizer2, device, num_epochs=phase2_epochs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a2750",
   "metadata": {},
   "source": [
    "**Two Phase Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c03e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting=True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = not feature_extracting\n",
    "\n",
    "def unfreeze_last_block(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer4\" in name or \"fc\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd038759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_epoch(model, dataloader, criterion, optimizer, device, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\" if train else \"Validation\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return acc, avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "314c7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_two_phase(model, train_loader, val_loader, criterion, device, num_epochs_phase1=5, num_epochs_phase2=5):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = model.state_dict()\n",
    "\n",
    "    # ------------------ Phase 1: Classifier only ------------------\n",
    "    print(\"\\nPhase 1: Training classifier only\")\n",
    "    set_parameter_requires_grad(model, feature_extracting=True)\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "    for epoch in range(num_epochs_phase1):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs_phase1} (Classifier Only)\")\n",
    "        train_acc, train_loss = run_one_epoch(model, train_loader, criterion, optimizer, device, train=True)\n",
    "        val_acc, val_loss = run_one_epoch(model, val_loader, criterion, optimizer, device, train=False)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "            print(\"Best model updated\")\n",
    "\n",
    "    # ------------------ Phase 2: Unfreeze last conv block ------------------\n",
    "    print(\"\\nPhase 2: Unfreezing last conv block\")\n",
    "    unfreeze_last_block(model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "    for epoch in range(num_epochs_phase2):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs_phase2} (Last Block + FC)\")\n",
    "        train_acc, train_loss = run_one_epoch(model, train_loader, criterion, optimizer, device, train=True)\n",
    "        val_acc, val_loss = run_one_epoch(model, val_loader, criterion, optimizer, device, train=False)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "            print(\"Best model updated\")\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613fa6a",
   "metadata": {},
   "source": [
    "**Efficient Net Training Two Phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ba294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_two_phase_en(model, train_loader, val_loader, criterion, device, num_epochs_phase1=5, num_epochs_phase2=5):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # --- Phase 1: Freeze backbone, train only classifier\n",
    "    print(\"\\nPhase 1: Training classifier only\")\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.backbone.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4)\n",
    "\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs_phase1, device)\n",
    "\n",
    "    # ---------- Phase 2: Unfreeze only last conv block ----------\n",
    "    print(\"\\nPhase 2: Fine-tuning classifier + last block only\")\n",
    "\n",
    "    # Unfreeze last block (EfficientNet blocks are in model.backbone.blocks)\n",
    "    for param in model.backbone.blocks[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Also keep classifier unfrozen\n",
    "    for param in model.backbone.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs_phase2, device)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ec9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = HAM10000Dataset(train_df, img_dirs, train_transform )\n",
    "# val_dataset = HAM10000Dataset(val_df, img_dirs, val_transform)\n",
    "\n",
    "# # Count how many samples in each class\n",
    "# class_counts = train_df['label'].value_counts().sort_index().values\n",
    "# class_weights = 1. / class_counts\n",
    "\n",
    "# # Map weights to each sample\n",
    "# sample_weights = train_df['label'].map(lambda x: class_weights[x]).values\n",
    "\n",
    "# # Create the sampler\n",
    "# sampler = WeightedRandomSampler(weights=sample_weights,\n",
    "#                                  num_samples=len(sample_weights),\n",
    "#                                  replacement=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b164ff27",
   "metadata": {},
   "source": [
    "**Device and Criterion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5688eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ca3bcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109380b6",
   "metadata": {},
   "source": [
    "**Two Phase ResNet Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472aee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_resnet50_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fad3e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 1: Training classifier only\n",
      "\n",
      "Epoch 1/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [04:05<00:00,  2.23s/it]\n",
      "Validation: 100%|██████████| 24/24 [01:00<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2886, Train Acc: 0.6355\n",
      "Val   Loss: 1.2538, Val   Acc: 0.6336\n",
      "Best model updated\n",
      "\n",
      "Epoch 2/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [02:55<00:00,  1.60s/it]\n",
      "Validation: 100%|██████████| 24/24 [01:07<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1094, Train Acc: 0.6767\n",
      "Val   Loss: 1.2168, Val   Acc: 0.6336\n",
      "\n",
      "Epoch 3/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [05:56<00:00,  3.24s/it]\n",
      "Validation: 100%|██████████| 24/24 [01:06<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0819, Train Acc: 0.6767\n",
      "Val   Loss: 1.1890, Val   Acc: 0.6336\n",
      "\n",
      "Epoch 4/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [05:18<00:00,  2.89s/it]\n",
      "Validation: 100%|██████████| 24/24 [01:06<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0563, Train Acc: 0.6767\n",
      "Val   Loss: 1.1671, Val   Acc: 0.6336\n",
      "\n",
      "Epoch 5/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [05:48<00:00,  3.17s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:46<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0321, Train Acc: 0.6767\n",
      "Val   Loss: 1.1409, Val   Acc: 0.6336\n",
      "\n",
      "Epoch 6/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [04:15<00:00,  2.32s/it]\n",
      "Validation: 100%|██████████| 24/24 [01:05<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0127, Train Acc: 0.6767\n",
      "Val   Loss: 1.1162, Val   Acc: 0.6356\n",
      "Best model updated\n",
      "\n",
      "Epoch 7/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [05:17<00:00,  2.89s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:35<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9909, Train Acc: 0.6770\n",
      "Val   Loss: 1.0962, Val   Acc: 0.6356\n",
      "\n",
      "Epoch 8/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:01<00:00,  1.65s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:35<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9708, Train Acc: 0.6771\n",
      "Val   Loss: 1.0782, Val   Acc: 0.6356\n",
      "\n",
      "Phase 2: Unfreezing last conv block\n",
      "\n",
      "Epoch 1/8 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:38<00:00,  1.98s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7879, Train Acc: 0.7168\n",
      "Val   Loss: 0.7730, Val   Acc: 0.7467\n",
      "Best model updated\n",
      "\n",
      "Epoch 2/8 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:38<00:00,  1.99s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6501, Train Acc: 0.7691\n",
      "Val   Loss: 0.6880, Val   Acc: 0.7573\n",
      "Best model updated\n",
      "\n",
      "Epoch 3/8 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:37<00:00,  1.98s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5595, Train Acc: 0.7960\n",
      "Val   Loss: 0.6375, Val   Acc: 0.7851\n",
      "Best model updated\n",
      "\n",
      "Epoch 4/8 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [04:46<00:00,  2.60s/it]\n",
      "Validation: 100%|██████████| 24/24 [01:12<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5074, Train Acc: 0.8145\n",
      "Val   Loss: 0.6318, Val   Acc: 0.7817\n",
      "\n",
      "Epoch 5/8 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:36<00:00,  1.97s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4624, Train Acc: 0.8347\n",
      "Val   Loss: 0.6225, Val   Acc: 0.7950\n",
      "Best model updated\n",
      "\n",
      "Epoch 6/8 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [04:05<00:00,  2.23s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4291, Train Acc: 0.8411\n",
      "Val   Loss: 0.6306, Val   Acc: 0.7910\n",
      "\n",
      "Epoch 7/8 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:38<00:00,  1.98s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4038, Train Acc: 0.8522\n",
      "Val   Loss: 0.6011, Val   Acc: 0.7910\n",
      "\n",
      "Epoch 8/8 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:37<00:00,  1.98s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3748, Train Acc: 0.8645\n",
      "Val   Loss: 0.5891, Val   Acc: 0.7989\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_model_two_phase(model, train_loader, val_loader, criterion, device, num_epochs_phase1=8, num_epochs_phase2=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31dba767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sabad\\OneDrive\\Desktop\\APS360\\HAM10K Dataset\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sabad\\OneDrive\\Desktop\\APS360\\HAM10K Dataset\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model2 = get_resnet50_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4746ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 1: Training classifier only\n",
      "\n",
      "Epoch 1/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:04<00:00,  1.68s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5448, Train Acc: 0.5225\n",
      "Val   Loss: 1.3417, Val   Acc: 0.6336\n",
      "Best model updated\n",
      "\n",
      "Epoch 2/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:06<00:00,  1.70s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1309, Train Acc: 0.6767\n",
      "Val   Loss: 1.2492, Val   Acc: 0.6336\n",
      "\n",
      "Epoch 3/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:06<00:00,  1.69s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0894, Train Acc: 0.6767\n",
      "Val   Loss: 1.2130, Val   Acc: 0.6336\n",
      "\n",
      "Epoch 4/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:06<00:00,  1.70s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:35<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0593, Train Acc: 0.6767\n",
      "Val   Loss: 1.1906, Val   Acc: 0.6336\n",
      "\n",
      "Epoch 5/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:06<00:00,  1.70s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0345, Train Acc: 0.6767\n",
      "Val   Loss: 1.1601, Val   Acc: 0.6343\n",
      "Best model updated\n",
      "\n",
      "Epoch 6/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:06<00:00,  1.69s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0215, Train Acc: 0.6767\n",
      "Val   Loss: 1.1406, Val   Acc: 0.6369\n",
      "Best model updated\n",
      "\n",
      "Epoch 7/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:06<00:00,  1.70s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0048, Train Acc: 0.6765\n",
      "Val   Loss: 1.1188, Val   Acc: 0.6362\n",
      "\n",
      "Epoch 8/8 (Classifier Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:06<00:00,  1.70s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9877, Train Acc: 0.6767\n",
      "Val   Loss: 1.1050, Val   Acc: 0.6396\n",
      "Best model updated\n",
      "\n",
      "Phase 2: Unfreezing last conv block\n",
      "\n",
      "Epoch 1/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.03s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7963, Train Acc: 0.7176\n",
      "Val   Loss: 0.7816, Val   Acc: 0.7361\n",
      "Best model updated\n",
      "\n",
      "Epoch 2/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6405, Train Acc: 0.7734\n",
      "Val   Loss: 0.6698, Val   Acc: 0.7626\n",
      "Best model updated\n",
      "\n",
      "Epoch 3/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5551, Train Acc: 0.7971\n",
      "Val   Loss: 0.6281, Val   Acc: 0.7765\n",
      "Best model updated\n",
      "\n",
      "Epoch 4/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5094, Train Acc: 0.8141\n",
      "Val   Loss: 0.5860, Val   Acc: 0.7864\n",
      "Best model updated\n",
      "\n",
      "Epoch 5/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4666, Train Acc: 0.8317\n",
      "Val   Loss: 0.5850, Val   Acc: 0.7930\n",
      "Best model updated\n",
      "\n",
      "Epoch 6/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4278, Train Acc: 0.8414\n",
      "Val   Loss: 0.6012, Val   Acc: 0.7857\n",
      "\n",
      "Epoch 7/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3970, Train Acc: 0.8562\n",
      "Val   Loss: 0.5771, Val   Acc: 0.7976\n",
      "Best model updated\n",
      "\n",
      "Epoch 8/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3759, Train Acc: 0.8629\n",
      "Val   Loss: 0.5856, Val   Acc: 0.8075\n",
      "Best model updated\n",
      "\n",
      "Epoch 9/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3582, Train Acc: 0.8696\n",
      "Val   Loss: 0.5936, Val   Acc: 0.7923\n",
      "\n",
      "Epoch 10/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3356, Train Acc: 0.8791\n",
      "Val   Loss: 0.5746, Val   Acc: 0.8102\n",
      "Best model updated\n",
      "\n",
      "Epoch 11/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3169, Train Acc: 0.8846\n",
      "Val   Loss: 0.6087, Val   Acc: 0.7903\n",
      "\n",
      "Epoch 12/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2924, Train Acc: 0.8935\n",
      "Val   Loss: 0.6206, Val   Acc: 0.8016\n",
      "\n",
      "Epoch 13/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2838, Train Acc: 0.8968\n",
      "Val   Loss: 0.5946, Val   Acc: 0.8016\n",
      "\n",
      "Epoch 14/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:42<00:00,  2.03s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2709, Train Acc: 0.9035\n",
      "Val   Loss: 0.6299, Val   Acc: 0.8016\n",
      "\n",
      "Epoch 15/15 (Last Block + FC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 110/110 [03:41<00:00,  2.02s/it]\n",
      "Validation: 100%|██████████| 24/24 [00:36<00:00,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2518, Train Acc: 0.9130\n",
      "Val   Loss: 0.6311, Val   Acc: 0.8069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = train_model_two_phase(model2, train_loader, val_loader, criterion, device, num_epochs_phase1=8, num_epochs_phase2=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868c15f",
   "metadata": {},
   "source": [
    "**Efficient Net Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2bbfda1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:58<00:00,  1.87it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 126.4991, Train Acc: 0.6109\n",
      "Val   Loss: 19.1609, Val   Acc: 0.7066\n",
      "Best model updated\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:56<00:00,  1.92it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 58.9989, Train Acc: 0.8031\n",
      "Val   Loss: 16.8356, Val   Acc: 0.7341\n",
      "Best model updated\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 19/109 [00:10<00:49,  1.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m eff_net = EfficientNetFinetune(num_classes=\u001b[32m7\u001b[39m)\n\u001b[32m      2\u001b[39m optimizer = torch.optim.Adam(eff_net.parameters(), lr=\u001b[32m1e-4\u001b[39m, weight_decay=\u001b[32m1e-3\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m eff_net = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43meff_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[39m\n\u001b[32m     21\u001b[39m     loss.backward()\n\u001b[32m     22\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     train_correct += (outputs.argmax(\u001b[32m1\u001b[39m) == labels).sum().item()\n\u001b[32m     27\u001b[39m train_acc = train_correct / \u001b[38;5;28mlen\u001b[39m(train_loader.dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "eff_net = EfficientNetFinetune(num_classes=7)\n",
    "optimizer = torch.optim.Adam(eff_net.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "eff_net = train_model(eff_net, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdee74",
   "metadata": {},
   "source": [
    "**Two Phase Efficient Net Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 1: Training classifier only\n",
      "\n",
      "Epoch 1/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:26<00:00,  2.52it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:17<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 251.6243, Train Acc: 0.6607\n",
      "Val   Loss: 56.5759, Val   Acc: 0.6369\n",
      "Best model updated\n",
      "\n",
      "Epoch 2/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:34<00:00,  2.32it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:15<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 199.7588, Train Acc: 0.7067\n",
      "Val   Loss: 51.0701, Val   Acc: 0.6700\n",
      "Best model updated\n",
      "\n",
      "Epoch 3/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:26<00:00,  2.52it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:15<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 185.8001, Train Acc: 0.7146\n",
      "Val   Loss: 48.9200, Val   Acc: 0.6726\n",
      "Best model updated\n",
      "\n",
      "Epoch 4/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:26<00:00,  2.53it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:17<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 176.2818, Train Acc: 0.7248\n",
      "Val   Loss: 48.1479, Val   Acc: 0.6759\n",
      "Best model updated\n",
      "\n",
      "Epoch 5/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:39<00:00,  2.20it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:18<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 169.8980, Train Acc: 0.7309\n",
      "Val   Loss: 45.3714, Val   Acc: 0.6984\n",
      "Best model updated\n",
      "\n",
      "Phase 2: Fine-tuning classifier + last block only\n",
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:43<00:00,  2.11it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:18<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 156.2549, Train Acc: 0.7455\n",
      "Val   Loss: 36.6331, Val   Acc: 0.7493\n",
      "Best model updated\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:43<00:00,  2.11it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:18<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 142.0548, Train Acc: 0.7656\n",
      "Val   Loss: 35.0072, Val   Acc: 0.7553\n",
      "Best model updated\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:38<00:00,  2.23it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:15<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 132.4922, Train Acc: 0.7810\n",
      "Val   Loss: 32.2194, Val   Acc: 0.7652\n",
      "Best model updated\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:43<00:00,  2.11it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:18<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 124.0202, Train Acc: 0.7918\n",
      "Val   Loss: 31.6469, Val   Acc: 0.7712\n",
      "Best model updated\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:43<00:00,  2.11it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:18<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 121.0269, Train Acc: 0.7995\n",
      "Val   Loss: 31.0318, Val   Acc: 0.7758\n",
      "Best model updated\n",
      "\n",
      "Epoch 6/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:44<00:00,  2.10it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:18<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 117.0657, Train Acc: 0.8064\n",
      "Val   Loss: 30.7574, Val   Acc: 0.7791\n",
      "Best model updated\n",
      "\n",
      "Epoch 7/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:44<00:00,  2.10it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:18<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 113.3911, Train Acc: 0.8094\n",
      "Val   Loss: 30.9627, Val   Acc: 0.7712\n",
      "\n",
      "Epoch 8/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:44<00:00,  2.10it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:18<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 109.8897, Train Acc: 0.8161\n",
      "Val   Loss: 29.6995, Val   Acc: 0.7804\n",
      "Best model updated\n",
      "\n",
      "Epoch 9/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:44<00:00,  2.09it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:18<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 105.4586, Train Acc: 0.8252\n",
      "Val   Loss: 29.6436, Val   Acc: 0.7851\n",
      "Best model updated\n",
      "\n",
      "Epoch 10/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 219/219 [01:44<00:00,  2.09it/s]\n",
      "Validation: 100%|██████████| 48/48 [00:18<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 101.9395, Train Acc: 0.8334\n",
      "Val   Loss: 29.7680, Val   Acc: 0.7877\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eff_net2 = EfficientNetFinetune(num_classes=7)\n",
    "eff_net2 = train_model_two_phase_en(eff_net2, train_loader, val_loader, criterion, device, num_epochs_phase1=10, num_epochs_phase2=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d972e6",
   "metadata": {},
   "source": [
    "**Fusion Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "041871bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:44<00:00,  2.47it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7034, Train Acc: 0.3440\n",
      "Val   Loss: 1.5678, Val   Acc: 0.4622\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 2/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3269, Train Acc: 0.5454\n",
      "Val   Loss: 1.3239, Val   Acc: 0.5700\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 3/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1924, Train Acc: 0.5941\n",
      "Val   Loss: 1.2602, Val   Acc: 0.5820\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 4/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0815, Train Acc: 0.6343\n",
      "Val   Loss: 1.1419, Val   Acc: 0.6155\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 5/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0213, Train Acc: 0.6500\n",
      "Val   Loss: 1.1016, Val   Acc: 0.6209\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 6/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9469, Train Acc: 0.6841\n",
      "Val   Loss: 1.0494, Val   Acc: 0.6430\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 7/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9173, Train Acc: 0.6895\n",
      "Val   Loss: 1.0249, Val   Acc: 0.6370\n",
      "\n",
      "Epoch 8/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8773, Train Acc: 0.6977\n",
      "Val   Loss: 1.0392, Val   Acc: 0.6303\n",
      "\n",
      "Epoch 9/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:45<00:00,  2.40it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8526, Train Acc: 0.6984\n",
      "Val   Loss: 1.0022, Val   Acc: 0.6383\n",
      "\n",
      "Epoch 10/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:46<00:00,  2.35it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8138, Train Acc: 0.7204\n",
      "Val   Loss: 0.9865, Val   Acc: 0.6484\n",
      "✅ Best model updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fusion_model = FusionModel(num_classes=7)\n",
    "freeze_all_but_meta_and_classifier(fusion_model)\n",
    "unfreeze_last_block(fusion_model)\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "fusion_model = train_fusion_model(fusion_model, train_loader, val_loader, criterion, optimizer, device=device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f654fb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 1: Training metadata branch and fusion classifier\n",
      "\n",
      "Epoch 1/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:52<00:00,  2.07it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2234, Train Acc: 0.5497\n",
      "Val   Loss: 1.0933, Val   Acc: 0.6008\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 2/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9393, Train Acc: 0.6514\n",
      "Val   Loss: 0.9860, Val   Acc: 0.6343\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 3/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8599, Train Acc: 0.6901\n",
      "Val   Loss: 1.0099, Val   Acc: 0.6142\n",
      "\n",
      "Epoch 4/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8019, Train Acc: 0.7000\n",
      "Val   Loss: 0.8993, Val   Acc: 0.6557\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 5/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7816, Train Acc: 0.7122\n",
      "Val   Loss: 0.9737, Val   Acc: 0.6189\n",
      "\n",
      "Epoch 6/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7739, Train Acc: 0.7138\n",
      "Val   Loss: 0.9045, Val   Acc: 0.6510\n",
      "\n",
      "Epoch 7/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7456, Train Acc: 0.7237\n",
      "Val   Loss: 0.8776, Val   Acc: 0.6638\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 8/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7187, Train Acc: 0.7326\n",
      "Val   Loss: 0.8954, Val   Acc: 0.6571\n",
      "\n",
      "Epoch 9/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6926, Train Acc: 0.7414\n",
      "Val   Loss: 0.8315, Val   Acc: 0.6792\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 10/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7143, Train Acc: 0.7329\n",
      "Val   Loss: 0.8794, Val   Acc: 0.6597\n",
      "\n",
      "Phase 2: Fine-tune last EfficientNet block with rest of model\n",
      "\n",
      "Epoch 1/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7110, Train Acc: 0.7322\n",
      "Val   Loss: 0.8688, Val   Acc: 0.6651\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 2/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7351, Train Acc: 0.7288\n",
      "Val   Loss: 0.8209, Val   Acc: 0.6825\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 3/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6953, Train Acc: 0.7427\n",
      "Val   Loss: 0.8451, Val   Acc: 0.6778\n",
      "\n",
      "Epoch 4/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7051, Train Acc: 0.7354\n",
      "Val   Loss: 0.8423, Val   Acc: 0.6792\n",
      "\n",
      "Epoch 5/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6969, Train Acc: 0.7418\n",
      "Val   Loss: 0.8373, Val   Acc: 0.6825\n",
      "\n",
      "Epoch 6/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6826, Train Acc: 0.7456\n",
      "Val   Loss: 0.8597, Val   Acc: 0.6718\n",
      "\n",
      "Epoch 7/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7079, Train Acc: 0.7411\n",
      "Val   Loss: 0.8442, Val   Acc: 0.6745\n",
      "\n",
      "Epoch 8/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6799, Train Acc: 0.7490\n",
      "Val   Loss: 0.8292, Val   Acc: 0.6772\n",
      "\n",
      "Epoch 9/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.49it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6901, Train Acc: 0.7466\n",
      "Val   Loss: 0.8426, Val   Acc: 0.6758\n",
      "\n",
      "Epoch 10/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.49it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6614, Train Acc: 0.7538\n",
      "Val   Loss: 0.8417, Val   Acc: 0.6765\n",
      "\n",
      "Epoch 11/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6928, Train Acc: 0.7436\n",
      "Val   Loss: 0.8367, Val   Acc: 0.6825\n",
      "\n",
      "Epoch 12/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6789, Train Acc: 0.7449\n",
      "Val   Loss: 0.8235, Val   Acc: 0.6805\n",
      "\n",
      "Epoch 13/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6769, Train Acc: 0.7497\n",
      "Val   Loss: 0.8163, Val   Acc: 0.6845\n",
      "✅ Best model updated!\n",
      "\n",
      "Epoch 14/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6710, Train Acc: 0.7483\n",
      "Val   Loss: 0.8252, Val   Acc: 0.6832\n",
      "\n",
      "Epoch 15/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n",
      "Validation: 100%|██████████| 24/24 [00:07<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6894, Train Acc: 0.7450\n",
      "Val   Loss: 0.8110, Val   Acc: 0.6906\n",
      "✅ Best model updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fusion_model = FusionModel(num_classes=7)\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "fusion_model = train_two_phase_fusion(fusion_model, train_loader, val_loader, device, phase1_epochs=10, phase2_epochs=15, wd=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa798f5e",
   "metadata": {},
   "source": [
    "**Image Only Training (EffNet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e335ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_image_only(model, train_loader, val_loader, device, num_epochs=5, lr=1e-4, wd=1e-5):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n[Image-Only] Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # ----- Train -----\n",
    "        model.train()\n",
    "        total_loss, correct = 0, 0\n",
    "\n",
    "        for images, _, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, _, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54311d9",
   "metadata": {},
   "source": [
    "**Freeze EffNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c6e21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_image_model(model):\n",
    "    for param in model.image_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.meta_net.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa7678",
   "metadata": {},
   "source": [
    "**Fusion Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09329f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fusion(model, train_loader, val_loader, device, num_epochs=5, lr=1e-4, wd=1e-5):\n",
    "    freeze_image_model(model)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n[Fusion Phase] Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # ----- Train -----\n",
    "        model.train()\n",
    "        total_loss, correct = 0, 0\n",
    "\n",
    "        for images, metadata, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            images, metadata, labels = images.to(device), metadata.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, metadata)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, metadata, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                images, metadata, labels = images.to(device), metadata.to(device), labels.to(device)\n",
    "                outputs = model(images, metadata)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1eacc1",
   "metadata": {},
   "source": [
    "**Training CNN First, Then Fusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Pretrain EfficientNet on image data\n",
    "image_model = EfficientNetFinetune(num_classes=7)\n",
    "image_model = train_image_only(image_model, train_loader, val_loader, device, num_epochs=10, wd=1e-4)\n",
    "\n",
    "# Step 2: Initialize FusionModel and copy pretrained image model\n",
    "fusion_model = FusionModel(image_model_name='efficientnet_b0', meta_input_dim=meta_input_dim, num_classes=7)\n",
    "fusion_model.image_model.load_state_dict(image_model.state_dict())\n",
    "\n",
    "# Step 3: Train FusionModel (freeze EfficientNet)\n",
    "fusion_model = train_fusion(fusion_model, train_loader, val_loader, device, num_epochs=10, wd=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b400d66",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fd1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, image_model_name='efficientnet_b2', meta_input_dim=19, num_classes=7):\n",
    "        super(FusionModel, self).__init__()\n",
    "        \n",
    "        # Image branch (pretrained EfficientNet-B2)\n",
    "        self.image_model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "        \n",
    "        # Get the correct number of input features for B2\n",
    "        in_features = 1408  # EfficientNet-B2 has 1408 output features before classifier\n",
    "        self.image_model.classifier = nn.Identity()  # Remove original classifier\n",
    "        \n",
    "        # Metadata branch\n",
    "        self.meta_net = nn.Sequential(\n",
    "            nn.Linear(meta_input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Fusion classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features + 32, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, image, metadata):\n",
    "        img_feat = self.image_model(image)\n",
    "        meta_feat = self.meta_net(metadata)\n",
    "        combined = torch.cat([img_feat, meta_feat], dim=1)\n",
    "        return self.classifier(combined)\n",
    "    \n",
    "    def freeze_cnn(self):\n",
    "        \"\"\"Freeze all CNN parameters\"\"\"\n",
    "        for param in self.image_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_classifier(self):\n",
    "        \"\"\"Unfreeze only classifier parameters\"\"\"\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.meta_net.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_first(model, train_loader, val_loader, epochs=10, device='cuda'):\n",
    "    \"\"\"Phase 1: Train only the CNN part on images\"\"\"\n",
    "    # Create temporary classifier for phase 1 training\n",
    "    temp_classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),  # Slightly higher dropout for B2\n",
    "        nn.Linear(1408, model.classifier[-1].out_features)\n",
    "    )\n",
    "    model.image_model.classifier = temp_classifier\n",
    "    \n",
    "    # Freeze metadata branch and fusion classifier\n",
    "    for param in model.meta_net.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.image_model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_weights = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for images, _, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.image_model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = accuracy_score(all_targets, all_preds)\n",
    "        train_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc, val_f1 = validate(model, val_loader, criterion, device, phase='image')\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_weights = model.image_model.state_dict().copy()\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, 'best_model.pth')\n",
    "            print(f\"Saved new best model with val_acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Restore best weights and original classifier structure\n",
    "    model.image_model.load_state_dict(best_model_weights)\n",
    "    model.image_model.classifier = nn.Identity()  # Restore to original configuration\n",
    "    return model\n",
    "\n",
    "def train_fusion_model(model, train_loader, val_loader, epochs=10, device='cuda'):\n",
    "    \"\"\"Phase 2: Train fusion model with frozen CNN\"\"\"\n",
    "\n",
    "    cnn_checkpoint = torch.load('best_cnn_model.pth')\n",
    "    cnn_state_dict = {}\n",
    "    \n",
    "    # Filter only CNN weights\n",
    "    for k, v in cnn_checkpoint['model_state_dict'].items():\n",
    "        if k.startswith('image_model.'):\n",
    "            cnn_state_dict[k.replace('image_model.', '')] = v\n",
    "    \n",
    "    # Load CNN weights\n",
    "    model.image_model.load_state_dict(cnn_state_dict)\n",
    "    \n",
    "    model.freeze_cnn()\n",
    "    model.unfreeze_classifier()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.meta_net.parameters()},\n",
    "        {'params': model.classifier.parameters()}\n",
    "    ], lr=1e-3, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_weights = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for images, metadata, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "            images = images.to(device)\n",
    "            metadata = metadata.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, metadata)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = accuracy_score(all_targets, all_preds)\n",
    "        train_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc, val_f1 = validate(model, val_loader, criterion, device, phase='fusion')\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_weights = model.image_model.state_dict().copy()\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, 'best_model_two.pth')\n",
    "            print(f\"Saved new best model with val_acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Restore best weights\n",
    "    if best_model_weights:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    return model\n",
    "\n",
    "def validate(model, val_loader, criterion, device, phase='fusion'):\n",
    "    \"\"\"Validation function for both phases\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if phase == 'image':\n",
    "                images, _, targets = batch\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                outputs = model.image_model(images)\n",
    "            else:  # fusion\n",
    "                images, metadata, targets = batch\n",
    "                images = images.to(device)\n",
    "                metadata = metadata.to(device).float()\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(images, metadata)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    val_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    return val_loss, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "456f378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model on test set and return metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, metadata, targets in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            metadata = metadata.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(images, metadata)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_probabilities.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = accuracy_score(all_targets, all_preds)\n",
    "    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_loss:.4f}\")\n",
    "    print(f\"Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'loss': test_loss,\n",
    "        'accuracy': test_acc,\n",
    "        'f1': test_f1,\n",
    "        'predictions': all_preds,\n",
    "        'probabilities': all_probabilities,\n",
    "        'targets': all_targets,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7db72e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 109/109 [00:56<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.2166 | Acc: 0.5869 | F1: 0.5802\n",
      "Val Loss: 0.8372 | Acc: 0.6952 | F1: 0.7222\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 109/109 [00:56<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 0.5612 | Acc: 0.8031 | F1: 0.8014\n",
      "Val Loss: 0.6746 | Acc: 0.7468 | F1: 0.7672\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 109/109 [00:56<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 0.4076 | Acc: 0.8551 | F1: 0.8541\n",
      "Val Loss: 0.6368 | Acc: 0.7736 | F1: 0.7907\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 109/109 [00:56<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 0.3227 | Acc: 0.8841 | F1: 0.8836\n",
      "Val Loss: 0.6849 | Acc: 0.7642 | F1: 0.7818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 109/109 [00:55<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Train Loss: 0.2556 | Acc: 0.9050 | F1: 0.9046\n",
      "Val Loss: 0.5918 | Acc: 0.8118 | F1: 0.8189\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 109/109 [00:55<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Train Loss: 0.2329 | Acc: 0.9155 | F1: 0.9152\n",
      "Val Loss: 0.7142 | Acc: 0.7636 | F1: 0.7838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 109/109 [00:54<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Train Loss: 0.1982 | Acc: 0.9290 | F1: 0.9288\n",
      "Val Loss: 0.6680 | Acc: 0.7964 | F1: 0.8062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 109/109 [00:55<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Train Loss: 0.1732 | Acc: 0.9363 | F1: 0.9360\n",
      "Val Loss: 0.6271 | Acc: 0.8185 | F1: 0.8235\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 109/109 [00:54<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Train Loss: 0.1444 | Acc: 0.9475 | F1: 0.9473\n",
      "Val Loss: 0.6453 | Acc: 0.8125 | F1: 0.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 109/109 [00:54<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Train Loss: 0.1322 | Acc: 0.9523 | F1: 0.9522\n",
      "Val Loss: 0.7124 | Acc: 0.7830 | F1: 0.7997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 109/109 [00:44<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.2844 | Acc: 0.9327 | F1: 0.9326\n",
      "Val Loss: 0.6111 | Acc: 0.8051 | F1: 0.8161\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 0.1507 | Acc: 0.9530 | F1: 0.9529\n",
      "Val Loss: 0.6721 | Acc: 0.8098 | F1: 0.8201\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 0.1465 | Acc: 0.9547 | F1: 0.9545\n",
      "Val Loss: 0.6894 | Acc: 0.8104 | F1: 0.8183\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 0.1417 | Acc: 0.9552 | F1: 0.9551\n",
      "Val Loss: 0.6852 | Acc: 0.8098 | F1: 0.8176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 109/109 [00:50<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Train Loss: 0.1372 | Acc: 0.9544 | F1: 0.9543\n",
      "Val Loss: 0.7125 | Acc: 0.8111 | F1: 0.8203\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 109/109 [00:44<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Train Loss: 0.1404 | Acc: 0.9517 | F1: 0.9517\n",
      "Val Loss: 0.7007 | Acc: 0.8044 | F1: 0.8153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Train Loss: 0.1257 | Acc: 0.9567 | F1: 0.9566\n",
      "Val Loss: 0.7190 | Acc: 0.8125 | F1: 0.8207\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Train Loss: 0.1265 | Acc: 0.9537 | F1: 0.9538\n",
      "Val Loss: 0.7154 | Acc: 0.8171 | F1: 0.8246\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Train Loss: 0.1306 | Acc: 0.9527 | F1: 0.9525\n",
      "Val Loss: 0.7548 | Acc: 0.8084 | F1: 0.8182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Train Loss: 0.1305 | Acc: 0.9567 | F1: 0.9567\n",
      "Val Loss: 0.6936 | Acc: 0.8192 | F1: 0.8257\n",
      "Saved best model\n"
     ]
    }
   ],
   "source": [
    "model = FusionModel(meta_input_dim=19, num_classes=7)\n",
    "model = train_cnn_first(model, train_loader, val_loader, epochs=10)\n",
    "model = train_fusion_model(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7357fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 24/24 [00:15<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Loss: 0.7121\n",
      "Accuracy: 0.8024\n",
      "F1 Score: 0.8090\n",
      "Final Test Accuracy: 80.24%\n",
      "Final Test F1 Score: 0.8090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = test_model(model, test_loader)\n",
    "\n",
    "# You can access individual results:\n",
    "print(f\"Final Test Accuracy: {results['accuracy']:.2%}\")\n",
    "print(f\"Final Test F1 Score: {results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03a20076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "def test_model(model, test_loader, class_names, device='cuda'):\n",
    "    \"\"\"Evaluate model with comprehensive per-class and overall metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, metadata, targets in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            metadata = metadata.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(images, metadata)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_probabilities.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(all_targets)\n",
    "    y_pred = np.array(all_preds)\n",
    "    y_probs = np.array(all_probabilities)\n",
    "    \n",
    "    # Number of classes\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    # Binarize for AUC calculation\n",
    "    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "    \n",
    "    # Calculate metrics for each class\n",
    "    metrics = {\n",
    "        'class_names': class_names,\n",
    "        'per_class': {},\n",
    "        'overall': {}\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    for i in range(n_classes):\n",
    "        metrics['per_class'][class_names[i]] = {\n",
    "            'accuracy': np.mean(y_true[y_true == i] == y_pred[y_true == i]),\n",
    "            'precision': precision_score(y_true, y_pred, labels=[i], average='micro'),\n",
    "            'recall': recall_score(y_true, y_pred, labels=[i], average='micro'),\n",
    "            'f1': f1_score(y_true, y_pred, labels=[i], average='micro'),\n",
    "            'auc': roc_auc_score(\n",
    "                y_true_bin[:, i],\n",
    "                y_probs[:, i],\n",
    "                multi_class='ovr'\n",
    "            ) if n_classes > 2 else roc_auc_score(y_true, y_probs[:, 1])\n",
    "        }\n",
    "    \n",
    "    # Overall metrics (averages)\n",
    "    metrics['overall'] = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro'),\n",
    "        'precision_weighted': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro'),\n",
    "        'recall_weighted': recall_score(y_true, y_pred, average='weighted'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'auc_macro': roc_auc_score(\n",
    "            y_true_bin,\n",
    "            y_probs,\n",
    "            multi_class='ovr',\n",
    "            average='macro'\n",
    "        ) if n_classes > 2 else roc_auc_score(y_true, y_probs[:, 1]),\n",
    "        'auc_weighted': roc_auc_score(\n",
    "            y_true_bin,\n",
    "            y_probs,\n",
    "            multi_class='ovr',\n",
    "            average='weighted'\n",
    "        ) if n_classes > 2 else roc_auc_score(y_true, y_probs[:, 1])\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n=== PER-CLASS METRICS ===\")\n",
    "    for cls in metrics['per_class']:\n",
    "        print(f\"\\nClass: {cls}\")\n",
    "        print(f\"Accuracy: {metrics['per_class'][cls]['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {metrics['per_class'][cls]['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['per_class'][cls]['recall']:.4f}\")\n",
    "        print(f\"F1: {metrics['per_class'][cls]['f1']:.4f}\")\n",
    "        print(f\"AUC: {metrics['per_class'][cls]['auc']:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== OVERALL METRICS ===\")\n",
    "    print(f\"Accuracy: {metrics['overall']['accuracy']:.4f}\")\n",
    "    print(f\"\\nMacro Averages:\")\n",
    "    print(f\"Precision: {metrics['overall']['precision_macro']:.4f}\")\n",
    "    print(f\"Recall: {metrics['overall']['recall_macro']:.4f}\")\n",
    "    print(f\"F1: {metrics['overall']['f1_macro']:.4f}\")\n",
    "    print(f\"AUC: {metrics['overall']['auc_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWeighted Averages:\")\n",
    "    print(f\"Precision: {metrics['overall']['precision_weighted']:.4f}\")\n",
    "    print(f\"Recall: {metrics['overall']['recall_weighted']:.4f}\")\n",
    "    print(f\"F1: {metrics['overall']['f1_weighted']:.4f}\")\n",
    "    print(f\"AUC: {metrics['overall']['auc_weighted']:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef638af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 24/24 [00:07<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PER-CLASS METRICS ===\n",
      "\n",
      "Class: nv\n",
      "Accuracy: 0.7045\n",
      "Precision: 0.6327\n",
      "Recall: 0.7045\n",
      "F1: 0.6667\n",
      "AUC: 0.9379\n",
      "\n",
      "Class: mel\n",
      "Accuracy: 0.8855\n",
      "Precision: 0.9312\n",
      "Recall: 0.8855\n",
      "F1: 0.9078\n",
      "AUC: 0.9507\n",
      "\n",
      "Class: bkl\n",
      "Accuracy: 0.5556\n",
      "Precision: 0.3125\n",
      "Recall: 0.5556\n",
      "F1: 0.4000\n",
      "AUC: 0.9891\n",
      "\n",
      "Class: bcc\n",
      "Accuracy: 0.6257\n",
      "Precision: 0.4908\n",
      "Recall: 0.6257\n",
      "F1: 0.5501\n",
      "AUC: 0.8960\n",
      "\n",
      "Class: akiec\n",
      "Accuracy: 0.7308\n",
      "Precision: 0.7600\n",
      "Recall: 0.7308\n",
      "F1: 0.7451\n",
      "AUC: 0.9789\n",
      "\n",
      "Class: vasc\n",
      "Accuracy: 0.6211\n",
      "Precision: 0.8310\n",
      "Recall: 0.6211\n",
      "F1: 0.7108\n",
      "AUC: 0.9734\n",
      "\n",
      "Class: df\n",
      "Accuracy: 0.5660\n",
      "Precision: 0.5769\n",
      "Recall: 0.5660\n",
      "F1: 0.5714\n",
      "AUC: 0.9488\n",
      "\n",
      "=== OVERALL METRICS ===\n",
      "Accuracy: 0.8024\n",
      "\n",
      "Macro Averages:\n",
      "Precision: 0.6479\n",
      "Recall: 0.6699\n",
      "F1: 0.6503\n",
      "AUC: 0.9535\n",
      "\n",
      "Weighted Averages:\n",
      "Precision: 0.8210\n",
      "Recall: 0.8024\n",
      "F1: 0.8090\n",
      "AUC: 0.9451\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "class_names = ['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df']  # Your actual class names\n",
    "results = test_model(model, test_loader, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d696236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'fusion_model_weights.pth')\n",
    "\n",
    "# Load\n",
    "# model = FusionModel(meta_input_dim=19, num_classes=7)  # Recreate model architecture\n",
    "# model.load_state_dict(torch.load('fusion_model_weights.pth'))\n",
    "# model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbe237d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 109/109 [00:55<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 1.2060 | Acc: 0.5872 | F1: 0.5818\n",
      "Val Loss: 0.8148 | Acc: 0.6946 | F1: 0.7222\n",
      "Saved new best model with val_acc: 0.6946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 109/109 [00:57<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "Train Loss: 0.5641 | Acc: 0.8009 | F1: 0.7990\n",
      "Val Loss: 0.6780 | Acc: 0.7515 | F1: 0.7698\n",
      "Saved new best model with val_acc: 0.7515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 109/109 [00:55<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "Train Loss: 0.3965 | Acc: 0.8560 | F1: 0.8547\n",
      "Val Loss: 0.6607 | Acc: 0.7528 | F1: 0.7726\n",
      "Saved new best model with val_acc: 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "Train Loss: 0.3322 | Acc: 0.8777 | F1: 0.8772\n",
      "Val Loss: 0.6356 | Acc: 0.7709 | F1: 0.7892\n",
      "Saved new best model with val_acc: 0.7709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "Train Loss: 0.2545 | Acc: 0.9090 | F1: 0.9088\n",
      "Val Loss: 0.6121 | Acc: 0.7991 | F1: 0.8089\n",
      "Saved new best model with val_acc: 0.7991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "Train Loss: 0.2233 | Acc: 0.9165 | F1: 0.9162\n",
      "Val Loss: 0.6198 | Acc: 0.7997 | F1: 0.8077\n",
      "Saved new best model with val_acc: 0.7997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "Train Loss: 0.1951 | Acc: 0.9300 | F1: 0.9297\n",
      "Val Loss: 0.6381 | Acc: 0.8058 | F1: 0.8152\n",
      "Saved new best model with val_acc: 0.8058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "Train Loss: 0.1643 | Acc: 0.9431 | F1: 0.9430\n",
      "Val Loss: 0.6459 | Acc: 0.7944 | F1: 0.8084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "Train Loss: 0.1491 | Acc: 0.9498 | F1: 0.9496\n",
      "Val Loss: 0.6621 | Acc: 0.8031 | F1: 0.8157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train]: 100%|██████████| 109/109 [00:55<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "Train Loss: 0.1376 | Acc: 0.9513 | F1: 0.9511\n",
      "Val Loss: 0.5872 | Acc: 0.8379 | F1: 0.8404\n",
      "Saved new best model with val_acc: 0.8379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "Train Loss: 0.1173 | Acc: 0.9609 | F1: 0.9609\n",
      "Val Loss: 0.6517 | Acc: 0.8171 | F1: 0.8244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Train]: 100%|██████████| 109/109 [00:55<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "Train Loss: 0.1142 | Acc: 0.9615 | F1: 0.9614\n",
      "Val Loss: 0.6820 | Acc: 0.8098 | F1: 0.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "Train Loss: 0.1008 | Acc: 0.9656 | F1: 0.9656\n",
      "Val Loss: 0.7444 | Acc: 0.7971 | F1: 0.8100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "Train Loss: 0.0944 | Acc: 0.9661 | F1: 0.9661\n",
      "Val Loss: 0.6681 | Acc: 0.8178 | F1: 0.8234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "Train Loss: 0.0935 | Acc: 0.9665 | F1: 0.9665\n",
      "Val Loss: 0.6635 | Acc: 0.8279 | F1: 0.8302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "Train Loss: 0.0938 | Acc: 0.9672 | F1: 0.9672\n",
      "Val Loss: 0.7086 | Acc: 0.8212 | F1: 0.8288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "Train Loss: 0.0763 | Acc: 0.9743 | F1: 0.9742\n",
      "Val Loss: 0.7209 | Acc: 0.8285 | F1: 0.8340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "Train Loss: 0.0790 | Acc: 0.9717 | F1: 0.9716\n",
      "Val Loss: 0.6859 | Acc: 0.8326 | F1: 0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "Train Loss: 0.0798 | Acc: 0.9730 | F1: 0.9729\n",
      "Val Loss: 0.7067 | Acc: 0.8399 | F1: 0.8401\n",
      "Saved new best model with val_acc: 0.8399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Train]: 100%|██████████| 109/109 [00:54<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "Train Loss: 0.0725 | Acc: 0.9756 | F1: 0.9755\n",
      "Val Loss: 0.6872 | Acc: 0.8392 | F1: 0.8353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 0.2160 | Acc: 0.9619 | F1: 0.9619\n",
      "Val Loss: 0.5905 | Acc: 0.8346 | F1: 0.8375\n",
      "Saved new best model with val_acc: 0.8346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 109/109 [00:42<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "Train Loss: 0.0990 | Acc: 0.9747 | F1: 0.9746\n",
      "Val Loss: 0.6520 | Acc: 0.8299 | F1: 0.8342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "Train Loss: 0.0907 | Acc: 0.9746 | F1: 0.9745\n",
      "Val Loss: 0.6523 | Acc: 0.8359 | F1: 0.8368\n",
      "Saved new best model with val_acc: 0.8359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "Train Loss: 0.0772 | Acc: 0.9747 | F1: 0.9747\n",
      "Val Loss: 0.6543 | Acc: 0.8433 | F1: 0.8411\n",
      "Saved new best model with val_acc: 0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "Train Loss: 0.0735 | Acc: 0.9767 | F1: 0.9767\n",
      "Val Loss: 0.6875 | Acc: 0.8379 | F1: 0.8404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "Train Loss: 0.0655 | Acc: 0.9793 | F1: 0.9793\n",
      "Val Loss: 0.7074 | Acc: 0.8366 | F1: 0.8386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "Train Loss: 0.0697 | Acc: 0.9783 | F1: 0.9783\n",
      "Val Loss: 0.7268 | Acc: 0.8346 | F1: 0.8383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "Train Loss: 0.0699 | Acc: 0.9771 | F1: 0.9771\n",
      "Val Loss: 0.7064 | Acc: 0.8466 | F1: 0.8444\n",
      "Saved new best model with val_acc: 0.8466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "Train Loss: 0.0674 | Acc: 0.9773 | F1: 0.9773\n",
      "Val Loss: 0.7223 | Acc: 0.8433 | F1: 0.8432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "Train Loss: 0.0650 | Acc: 0.9780 | F1: 0.9780\n",
      "Val Loss: 0.7123 | Acc: 0.8500 | F1: 0.8487\n",
      "Saved new best model with val_acc: 0.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "Train Loss: 0.0655 | Acc: 0.9773 | F1: 0.9772\n",
      "Val Loss: 0.7455 | Acc: 0.8392 | F1: 0.8417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "Train Loss: 0.0591 | Acc: 0.9807 | F1: 0.9807\n",
      "Val Loss: 0.7328 | Acc: 0.8399 | F1: 0.8401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "Train Loss: 0.0667 | Acc: 0.9771 | F1: 0.9771\n",
      "Val Loss: 0.7417 | Acc: 0.8446 | F1: 0.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "Train Loss: 0.0593 | Acc: 0.9812 | F1: 0.9812\n",
      "Val Loss: 0.7663 | Acc: 0.8433 | F1: 0.8431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "Train Loss: 0.0591 | Acc: 0.9816 | F1: 0.9816\n",
      "Val Loss: 0.7825 | Acc: 0.8446 | F1: 0.8421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "Train Loss: 0.0634 | Acc: 0.9784 | F1: 0.9784\n",
      "Val Loss: 0.7952 | Acc: 0.8379 | F1: 0.8407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "Train Loss: 0.0625 | Acc: 0.9797 | F1: 0.9797\n",
      "Val Loss: 0.7697 | Acc: 0.8339 | F1: 0.8356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "Train Loss: 0.0553 | Acc: 0.9828 | F1: 0.9827\n",
      "Val Loss: 0.7736 | Acc: 0.8446 | F1: 0.8437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "Train Loss: 0.0554 | Acc: 0.9829 | F1: 0.9829\n",
      "Val Loss: 0.7651 | Acc: 0.8446 | F1: 0.8436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Train]: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "Train Loss: 0.0556 | Acc: 0.9828 | F1: 0.9827\n",
      "Val Loss: 0.7723 | Acc: 0.8285 | F1: 0.8318\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FusionModel:\n\tMissing key(s) in state_dict: \"image_model.features.0.0.weight\", \"image_model.features.0.1.weight\", \"image_model.features.0.1.bias\", \"image_model.features.0.1.running_mean\", \"image_model.features.0.1.running_var\", \"image_model.features.1.0.block.0.0.weight\", \"image_model.features.1.0.block.0.1.weight\", \"image_model.features.1.0.block.0.1.bias\", \"image_model.features.1.0.block.0.1.running_mean\", \"image_model.features.1.0.block.0.1.running_var\", \"image_model.features.1.0.block.1.fc1.weight\", \"image_model.features.1.0.block.1.fc1.bias\", \"image_model.features.1.0.block.1.fc2.weight\", \"image_model.features.1.0.block.1.fc2.bias\", \"image_model.features.1.0.block.2.0.weight\", \"image_model.features.1.0.block.2.1.weight\", \"image_model.features.1.0.block.2.1.bias\", \"image_model.features.1.0.block.2.1.running_mean\", \"image_model.features.1.0.block.2.1.running_var\", \"image_model.features.1.1.block.0.0.weight\", \"image_model.features.1.1.block.0.1.weight\", \"image_model.features.1.1.block.0.1.bias\", \"image_model.features.1.1.block.0.1.running_mean\", \"image_model.features.1.1.block.0.1.running_var\", \"image_model.features.1.1.block.1.fc1.weight\", \"image_model.features.1.1.block.1.fc1.bias\", \"image_model.features.1.1.block.1.fc2.weight\", \"image_model.features.1.1.block.1.fc2.bias\", \"image_model.features.1.1.block.2.0.weight\", \"image_model.features.1.1.block.2.1.weight\", \"image_model.features.1.1.block.2.1.bias\", \"image_model.features.1.1.block.2.1.running_mean\", \"image_model.features.1.1.block.2.1.running_var\", \"image_model.features.2.0.block.0.0.weight\", \"image_model.features.2.0.block.0.1.weight\", \"image_model.features.2.0.block.0.1.bias\", \"image_model.features.2.0.block.0.1.running_mean\", \"image_model.features.2.0.block.0.1.running_var\", \"image_model.features.2.0.block.1.0.weight\", \"image_model.features.2.0.block.1.1.weight\", \"image_model.features.2.0.block.1.1.bias\", \"image_model.features.2.0.block.1.1.running_mean\", \"image_model.features.2.0.block.1.1.running_var\", \"image_model.features.2.0.block.2.fc1.weight\", \"image_model.features.2.0.block.2.fc1.bias\", \"image_model.features.2.0.block.2.fc2.weight\", \"image_model.features.2.0.block.2.fc2.bias\", \"image_model.features.2.0.block.3.0.weight\", \"image_model.features.2.0.block.3.1.weight\", \"image_model.features.2.0.block.3.1.bias\", \"image_model.features.2.0.block.3.1.running_mean\", \"image_model.features.2.0.block.3.1.running_var\", \"image_model.features.2.1.block.0.0.weight\", \"image_model.features.2.1.block.0.1.weight\", \"image_model.features.2.1.block.0.1.bias\", \"image_model.features.2.1.block.0.1.running_mean\", \"image_model.features.2.1.block.0.1.running_var\", \"image_model.features.2.1.block.1.0.weight\", \"image_model.features.2.1.block.1.1.weight\", \"image_model.features.2.1.block.1.1.bias\", \"image_model.features.2.1.block.1.1.running_mean\", \"image_model.features.2.1.block.1.1.running_var\", \"image_model.features.2.1.block.2.fc1.weight\", \"image_model.features.2.1.block.2.fc1.bias\", \"image_model.features.2.1.block.2.fc2.weight\", \"image_model.features.2.1.block.2.fc2.bias\", \"image_model.features.2.1.block.3.0.weight\", \"image_model.features.2.1.block.3.1.weight\", \"image_model.features.2.1.block.3.1.bias\", \"image_model.features.2.1.block.3.1.running_mean\", \"image_model.features.2.1.block.3.1.running_var\", \"image_model.features.2.2.block.0.0.weight\", \"image_model.features.2.2.block.0.1.weight\", \"image_model.features.2.2.block.0.1.bias\", \"image_model.features.2.2.block.0.1.running_mean\", \"image_model.features.2.2.block.0.1.running_var\", \"image_model.features.2.2.block.1.0.weight\", \"image_model.features.2.2.block.1.1.weight\", \"image_model.features.2.2.block.1.1.bias\", \"image_model.features.2.2.block.1.1.running_mean\", \"image_model.features.2.2.block.1.1.running_var\", \"image_model.features.2.2.block.2.fc1.weight\", \"image_model.features.2.2.block.2.fc1.bias\", \"image_model.features.2.2.block.2.fc2.weight\", \"image_model.features.2.2.block.2.fc2.bias\", \"image_model.features.2.2.block.3.0.weight\", \"image_model.features.2.2.block.3.1.weight\", \"image_model.features.2.2.block.3.1.bias\", \"image_model.features.2.2.block.3.1.running_mean\", \"image_model.features.2.2.block.3.1.running_var\", \"image_model.features.3.0.block.0.0.weight\", \"image_model.features.3.0.block.0.1.weight\", \"image_model.features.3.0.block.0.1.bias\", \"image_model.features.3.0.block.0.1.running_mean\", \"image_model.features.3.0.block.0.1.running_var\", \"image_model.features.3.0.block.1.0.weight\", \"image_model.features.3.0.block.1.1.weight\", \"image_model.features.3.0.block.1.1.bias\", \"image_model.features.3.0.block.1.1.running_mean\", \"image_model.features.3.0.block.1.1.running_var\", \"image_model.features.3.0.block.2.fc1.weight\", \"image_model.features.3.0.block.2.fc1.bias\", \"image_model.features.3.0.block.2.fc2.weight\", \"image_model.features.3.0.block.2.fc2.bias\", \"image_model.features.3.0.block.3.0.weight\", \"image_model.features.3.0.block.3.1.weight\", \"image_model.features.3.0.block.3.1.bias\", \"image_model.features.3.0.block.3.1.running_mean\", \"image_model.features.3.0.block.3.1.running_var\", \"image_model.features.3.1.block.0.0.weight\", \"image_model.features.3.1.block.0.1.weight\", \"image_model.features.3.1.block.0.1.bias\", \"image_model.features.3.1.block.0.1.running_mean\", \"image_model.features.3.1.block.0.1.running_var\", \"image_model.features.3.1.block.1.0.weight\", \"image_model.features.3.1.block.1.1.weight\", \"image_model.features.3.1.block.1.1.bias\", \"image_model.features.3.1.block.1.1.running_mean\", \"image_model.features.3.1.block.1.1.running_var\", \"image_model.features.3.1.block.2.fc1.weight\", \"image_model.features.3.1.block.2.fc1.bias\", \"image_model.features.3.1.block.2.fc2.weight\", \"image_model.features.3.1.block.2.fc2.bias\", \"image_model.features.3.1.block.3.0.weight\", \"image_model.features.3.1.block.3.1.weight\", \"image_model.features.3.1.block.3.1.bias\", \"image_model.features.3.1.block.3.1.running_mean\", \"image_model.features.3.1.block.3.1.running_var\", \"image_model.features.3.2.block.0.0.weight\", \"image_model.features.3.2.block.0.1.weight\", \"image_model.features.3.2.block.0.1.bias\", \"image_model.features.3.2.block.0.1.running_mean\", \"image_model.features.3.2.block.0.1.running_var\", \"image_model.features.3.2.block.1.0.weight\", \"image_model.features.3.2.block.1.1.weight\", \"image_model.features.3.2.block.1.1.bias\", \"image_model.features.3.2.block.1.1.running_mean\", \"image_model.features.3.2.block.1.1.running_var\", \"image_model.features.3.2.block.2.fc1.weight\", \"image_model.features.3.2.block.2.fc1.bias\", \"image_model.features.3.2.block.2.fc2.weight\", \"image_model.features.3.2.block.2.fc2.bias\", \"image_model.features.3.2.block.3.0.weight\", \"image_model.features.3.2.block.3.1.weight\", \"image_model.features.3.2.block.3.1.bias\", \"image_model.features.3.2.block.3.1.running_mean\", \"image_model.features.3.2.block.3.1.running_var\", \"image_model.features.4.0.block.0.0.weight\", \"image_model.features.4.0.block.0.1.weight\", \"image_model.features.4.0.block.0.1.bias\", \"image_model.features.4.0.block.0.1.running_mean\", \"image_model.features.4.0.block.0.1.running_var\", \"image_model.features.4.0.block.1.0.weight\", \"image_model.features.4.0.block.1.1.weight\", \"image_model.features.4.0.block.1.1.bias\", \"image_model.features.4.0.block.1.1.running_mean\", \"image_model.features.4.0.block.1.1.running_var\", \"image_model.features.4.0.block.2.fc1.weight\", \"image_model.features.4.0.block.2.fc1.bias\", \"image_model.features.4.0.block.2.fc2.weight\", \"image_model.features.4.0.block.2.fc2.bias\", \"image_model.features.4.0.block.3.0.weight\", \"image_model.features.4.0.block.3.1.weight\", \"image_model.features.4.0.block.3.1.bias\", \"image_model.features.4.0.block.3.1.running_mean\", \"image_model.features.4.0.block.3.1.running_var\", \"image_model.features.4.1.block.0.0.weight\", \"image_model.features.4.1.block.0.1.weight\", \"image_model.features.4.1.block.0.1.bias\", \"image_model.features.4.1.block.0.1.running_mean\", \"image_model.features.4.1.block.0.1.running_var\", \"image_model.features.4.1.block.1.0.weight\", \"image_model.features.4.1.block.1.1.weight\", \"image_model.features.4.1.block.1.1.bias\", \"image_model.features.4.1.block.1.1.running_mean\", \"image_model.features.4.1.block.1.1.running_var\", \"image_model.features.4.1.block.2.fc1.weight\", \"image_model.features.4.1.block.2.fc1.bias\", \"image_model.features.4.1.block.2.fc2.weight\", \"image_model.features.4.1.block.2.fc2.bias\", \"image_model.features.4.1.block.3.0.weight\", \"image_model.features.4.1.block.3.1.weight\", \"image_model.features.4.1.block.3.1.bias\", \"image_model.features.4.1.block.3.1.running_mean\", \"image_model.features.4.1.block.3.1.running_var\", \"image_model.features.4.2.block.0.0.weight\", \"image_model.features.4.2.block.0.1.weight\", \"image_model.features.4.2.block.0.1.bias\", \"image_model.features.4.2.block.0.1.running_mean\", \"image_model.features.4.2.block.0.1.running_var\", \"image_model.features.4.2.block.1.0.weight\", \"image_model.features.4.2.block.1.1.weight\", \"image_model.features.4.2.block.1.1.bias\", \"image_model.features.4.2.block.1.1.running_mean\", \"image_model.features.4.2.block.1.1.running_var\", \"image_model.features.4.2.block.2.fc1.weight\", \"image_model.features.4.2.block.2.fc1.bias\", \"image_model.features.4.2.block.2.fc2.weight\", \"image_model.features.4.2.block.2.fc2.bias\", \"image_model.features.4.2.block.3.0.weight\", \"image_model.features.4.2.block.3.1.weight\", \"image_model.features.4.2.block.3.1.bias\", \"image_model.features.4.2.block.3.1.running_mean\", \"image_model.features.4.2.block.3.1.running_var\", \"image_model.features.4.3.block.0.0.weight\", \"image_model.features.4.3.block.0.1.weight\", \"image_model.features.4.3.block.0.1.bias\", \"image_model.features.4.3.block.0.1.running_mean\", \"image_model.features.4.3.block.0.1.running_var\", \"image_model.features.4.3.block.1.0.weight\", \"image_model.features.4.3.block.1.1.weight\", \"image_model.features.4.3.block.1.1.bias\", \"image_model.features.4.3.block.1.1.running_mean\", \"image_model.features.4.3.block.1.1.running_var\", \"image_model.features.4.3.block.2.fc1.weight\", \"image_model.features.4.3.block.2.fc1.bias\", \"image_model.features.4.3.block.2.fc2.weight\", \"image_model.features.4.3.block.2.fc2.bias\", \"image_model.features.4.3.block.3.0.weight\", \"image_model.features.4.3.block.3.1.weight\", \"image_model.features.4.3.block.3.1.bias\", \"image_model.features.4.3.block.3.1.running_mean\", \"image_model.features.4.3.block.3.1.running_var\", \"image_model.features.5.0.block.0.0.weight\", \"image_model.features.5.0.block.0.1.weight\", \"image_model.features.5.0.block.0.1.bias\", \"image_model.features.5.0.block.0.1.running_mean\", \"image_model.features.5.0.block.0.1.running_var\", \"image_model.features.5.0.block.1.0.weight\", \"image_model.features.5.0.block.1.1.weight\", \"image_model.features.5.0.block.1.1.bias\", \"image_model.features.5.0.block.1.1.running_mean\", \"image_model.features.5.0.block.1.1.running_var\", \"image_model.features.5.0.block.2.fc1.weight\", \"image_model.features.5.0.block.2.fc1.bias\", \"image_model.features.5.0.block.2.fc2.weight\", \"image_model.features.5.0.block.2.fc2.bias\", \"image_model.features.5.0.block.3.0.weight\", \"image_model.features.5.0.block.3.1.weight\", \"image_model.features.5.0.block.3.1.bias\", \"image_model.features.5.0.block.3.1.running_mean\", \"image_model.features.5.0.block.3.1.running_var\", \"image_model.features.5.1.block.0.0.weight\", \"image_model.features.5.1.block.0.1.weight\", \"image_model.features.5.1.block.0.1.bias\", \"image_model.features.5.1.block.0.1.running_mean\", \"image_model.features.5.1.block.0.1.running_var\", \"image_model.features.5.1.block.1.0.weight\", \"image_model.features.5.1.block.1.1.weight\", \"image_model.features.5.1.block.1.1.bias\", \"image_model.features.5.1.block.1.1.running_mean\", \"image_model.features.5.1.block.1.1.running_var\", \"image_model.features.5.1.block.2.fc1.weight\", \"image_model.features.5.1.block.2.fc1.bias\", \"image_model.features.5.1.block.2.fc2.weight\", \"image_model.features.5.1.block.2.fc2.bias\", \"image_model.features.5.1.block.3.0.weight\", \"image_model.features.5.1.block.3.1.weight\", \"image_model.features.5.1.block.3.1.bias\", \"image_model.features.5.1.block.3.1.running_mean\", \"image_model.features.5.1.block.3.1.running_var\", \"image_model.features.5.2.block.0.0.weight\", \"image_model.features.5.2.block.0.1.weight\", \"image_model.features.5.2.block.0.1.bias\", \"image_model.features.5.2.block.0.1.running_mean\", \"image_model.features.5.2.block.0.1.running_var\", \"image_model.features.5.2.block.1.0.weight\", \"image_model.features.5.2.block.1.1.weight\", \"image_model.features.5.2.block.1.1.bias\", \"image_model.features.5.2.block.1.1.running_mean\", \"image_model.features.5.2.block.1.1.running_var\", \"image_model.features.5.2.block.2.fc1.weight\", \"image_model.features.5.2.block.2.fc1.bias\", \"image_model.features.5.2.block.2.fc2.weight\", \"image_model.features.5.2.block.2.fc2.bias\", \"image_model.features.5.2.block.3.0.weight\", \"image_model.features.5.2.block.3.1.weight\", \"image_model.features.5.2.block.3.1.bias\", \"image_model.features.5.2.block.3.1.running_mean\", \"image_model.features.5.2.block.3.1.running_var\", \"image_model.features.5.3.block.0.0.weight\", \"image_model.features.5.3.block.0.1.weight\", \"image_model.features.5.3.block.0.1.bias\", \"image_model.features.5.3.block.0.1.running_mean\", \"image_model.features.5.3.block.0.1.running_var\", \"image_model.features.5.3.block.1.0.weight\", \"image_model.features.5.3.block.1.1.weight\", \"image_model.features.5.3.block.1.1.bias\", \"image_model.features.5.3.block.1.1.running_mean\", \"image_model.features.5.3.block.1.1.running_var\", \"image_model.features.5.3.block.2.fc1.weight\", \"image_model.features.5.3.block.2.fc1.bias\", \"image_model.features.5.3.block.2.fc2.weight\", \"image_model.features.5.3.block.2.fc2.bias\", \"image_model.features.5.3.block.3.0.weight\", \"image_model.features.5.3.block.3.1.weight\", \"image_model.features.5.3.block.3.1.bias\", \"image_model.features.5.3.block.3.1.running_mean\", \"image_model.features.5.3.block.3.1.running_var\", \"image_model.features.6.0.block.0.0.weight\", \"image_model.features.6.0.block.0.1.weight\", \"image_model.features.6.0.block.0.1.bias\", \"image_model.features.6.0.block.0.1.running_mean\", \"image_model.features.6.0.block.0.1.running_var\", \"image_model.features.6.0.block.1.0.weight\", \"image_model.features.6.0.block.1.1.weight\", \"image_model.features.6.0.block.1.1.bias\", \"image_model.features.6.0.block.1.1.running_mean\", \"image_model.features.6.0.block.1.1.running_var\", \"image_model.features.6.0.block.2.fc1.weight\", \"image_model.features.6.0.block.2.fc1.bias\", \"image_model.features.6.0.block.2.fc2.weight\", \"image_model.features.6.0.block.2.fc2.bias\", \"image_model.features.6.0.block.3.0.weight\", \"image_model.features.6.0.block.3.1.weight\", \"image_model.features.6.0.block.3.1.bias\", \"image_model.features.6.0.block.3.1.running_mean\", \"image_model.features.6.0.block.3.1.running_var\", \"image_model.features.6.1.block.0.0.weight\", \"image_model.features.6.1.block.0.1.weight\", \"image_model.features.6.1.block.0.1.bias\", \"image_model.features.6.1.block.0.1.running_mean\", \"image_model.features.6.1.block.0.1.running_var\", \"image_model.features.6.1.block.1.0.weight\", \"image_model.features.6.1.block.1.1.weight\", \"image_model.features.6.1.block.1.1.bias\", \"image_model.features.6.1.block.1.1.running_mean\", \"image_model.features.6.1.block.1.1.running_var\", \"image_model.features.6.1.block.2.fc1.weight\", \"image_model.features.6.1.block.2.fc1.bias\", \"image_model.features.6.1.block.2.fc2.weight\", \"image_model.features.6.1.block.2.fc2.bias\", \"image_model.features.6.1.block.3.0.weight\", \"image_model.features.6.1.block.3.1.weight\", \"image_model.features.6.1.block.3.1.bias\", \"image_model.features.6.1.block.3.1.running_mean\", \"image_model.features.6.1.block.3.1.running_var\", \"image_model.features.6.2.block.0.0.weight\", \"image_model.features.6.2.block.0.1.weight\", \"image_model.features.6.2.block.0.1.bias\", \"image_model.features.6.2.block.0.1.running_mean\", \"image_model.features.6.2.block.0.1.running_var\", \"image_model.features.6.2.block.1.0.weight\", \"image_model.features.6.2.block.1.1.weight\", \"image_model.features.6.2.block.1.1.bias\", \"image_model.features.6.2.block.1.1.running_mean\", \"image_model.features.6.2.block.1.1.running_var\", \"image_model.features.6.2.block.2.fc1.weight\", \"image_model.features.6.2.block.2.fc1.bias\", \"image_model.features.6.2.block.2.fc2.weight\", \"image_model.features.6.2.block.2.fc2.bias\", \"image_model.features.6.2.block.3.0.weight\", \"image_model.features.6.2.block.3.1.weight\", \"image_model.features.6.2.block.3.1.bias\", \"image_model.features.6.2.block.3.1.running_mean\", \"image_model.features.6.2.block.3.1.running_var\", \"image_model.features.6.3.block.0.0.weight\", \"image_model.features.6.3.block.0.1.weight\", \"image_model.features.6.3.block.0.1.bias\", \"image_model.features.6.3.block.0.1.running_mean\", \"image_model.features.6.3.block.0.1.running_var\", \"image_model.features.6.3.block.1.0.weight\", \"image_model.features.6.3.block.1.1.weight\", \"image_model.features.6.3.block.1.1.bias\", \"image_model.features.6.3.block.1.1.running_mean\", \"image_model.features.6.3.block.1.1.running_var\", \"image_model.features.6.3.block.2.fc1.weight\", \"image_model.features.6.3.block.2.fc1.bias\", \"image_model.features.6.3.block.2.fc2.weight\", \"image_model.features.6.3.block.2.fc2.bias\", \"image_model.features.6.3.block.3.0.weight\", \"image_model.features.6.3.block.3.1.weight\", \"image_model.features.6.3.block.3.1.bias\", \"image_model.features.6.3.block.3.1.running_mean\", \"image_model.features.6.3.block.3.1.running_var\", \"image_model.features.6.4.block.0.0.weight\", \"image_model.features.6.4.block.0.1.weight\", \"image_model.features.6.4.block.0.1.bias\", \"image_model.features.6.4.block.0.1.running_mean\", \"image_model.features.6.4.block.0.1.running_var\", \"image_model.features.6.4.block.1.0.weight\", \"image_model.features.6.4.block.1.1.weight\", \"image_model.features.6.4.block.1.1.bias\", \"image_model.features.6.4.block.1.1.running_mean\", \"image_model.features.6.4.block.1.1.running_var\", \"image_model.features.6.4.block.2.fc1.weight\", \"image_model.features.6.4.block.2.fc1.bias\", \"image_model.features.6.4.block.2.fc2.weight\", \"image_model.features.6.4.block.2.fc2.bias\", \"image_model.features.6.4.block.3.0.weight\", \"image_model.features.6.4.block.3.1.weight\", \"image_model.features.6.4.block.3.1.bias\", \"image_model.features.6.4.block.3.1.running_mean\", \"image_model.features.6.4.block.3.1.running_var\", \"image_model.features.7.0.block.0.0.weight\", \"image_model.features.7.0.block.0.1.weight\", \"image_model.features.7.0.block.0.1.bias\", \"image_model.features.7.0.block.0.1.running_mean\", \"image_model.features.7.0.block.0.1.running_var\", \"image_model.features.7.0.block.1.0.weight\", \"image_model.features.7.0.block.1.1.weight\", \"image_model.features.7.0.block.1.1.bias\", \"image_model.features.7.0.block.1.1.running_mean\", \"image_model.features.7.0.block.1.1.running_var\", \"image_model.features.7.0.block.2.fc1.weight\", \"image_model.features.7.0.block.2.fc1.bias\", \"image_model.features.7.0.block.2.fc2.weight\", \"image_model.features.7.0.block.2.fc2.bias\", \"image_model.features.7.0.block.3.0.weight\", \"image_model.features.7.0.block.3.1.weight\", \"image_model.features.7.0.block.3.1.bias\", \"image_model.features.7.0.block.3.1.running_mean\", \"image_model.features.7.0.block.3.1.running_var\", \"image_model.features.7.1.block.0.0.weight\", \"image_model.features.7.1.block.0.1.weight\", \"image_model.features.7.1.block.0.1.bias\", \"image_model.features.7.1.block.0.1.running_mean\", \"image_model.features.7.1.block.0.1.running_var\", \"image_model.features.7.1.block.1.0.weight\", \"image_model.features.7.1.block.1.1.weight\", \"image_model.features.7.1.block.1.1.bias\", \"image_model.features.7.1.block.1.1.running_mean\", \"image_model.features.7.1.block.1.1.running_var\", \"image_model.features.7.1.block.2.fc1.weight\", \"image_model.features.7.1.block.2.fc1.bias\", \"image_model.features.7.1.block.2.fc2.weight\", \"image_model.features.7.1.block.2.fc2.bias\", \"image_model.features.7.1.block.3.0.weight\", \"image_model.features.7.1.block.3.1.weight\", \"image_model.features.7.1.block.3.1.bias\", \"image_model.features.7.1.block.3.1.running_mean\", \"image_model.features.7.1.block.3.1.running_var\", \"image_model.features.8.0.weight\", \"image_model.features.8.1.weight\", \"image_model.features.8.1.bias\", \"image_model.features.8.1.running_mean\", \"image_model.features.8.1.running_var\", \"meta_net.0.weight\", \"meta_net.0.bias\", \"meta_net.1.weight\", \"meta_net.1.bias\", \"meta_net.1.running_mean\", \"meta_net.1.running_var\", \"meta_net.4.weight\", \"meta_net.4.bias\", \"meta_net.5.weight\", \"meta_net.5.bias\", \"meta_net.5.running_mean\", \"meta_net.5.running_var\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.1.weight\", \"classifier.1.bias\", \"classifier.1.running_mean\", \"classifier.1.running_var\", \"classifier.4.weight\", \"classifier.4.bias\". \n\tUnexpected key(s) in state_dict: \"features.0.0.weight\", \"features.0.1.weight\", \"features.0.1.bias\", \"features.0.1.running_mean\", \"features.0.1.running_var\", \"features.0.1.num_batches_tracked\", \"features.1.0.block.0.0.weight\", \"features.1.0.block.0.1.weight\", \"features.1.0.block.0.1.bias\", \"features.1.0.block.0.1.running_mean\", \"features.1.0.block.0.1.running_var\", \"features.1.0.block.0.1.num_batches_tracked\", \"features.1.0.block.1.fc1.weight\", \"features.1.0.block.1.fc1.bias\", \"features.1.0.block.1.fc2.weight\", \"features.1.0.block.1.fc2.bias\", \"features.1.0.block.2.0.weight\", \"features.1.0.block.2.1.weight\", \"features.1.0.block.2.1.bias\", \"features.1.0.block.2.1.running_mean\", \"features.1.0.block.2.1.running_var\", \"features.1.0.block.2.1.num_batches_tracked\", \"features.1.1.block.0.0.weight\", \"features.1.1.block.0.1.weight\", \"features.1.1.block.0.1.bias\", \"features.1.1.block.0.1.running_mean\", \"features.1.1.block.0.1.running_var\", \"features.1.1.block.0.1.num_batches_tracked\", \"features.1.1.block.1.fc1.weight\", \"features.1.1.block.1.fc1.bias\", \"features.1.1.block.1.fc2.weight\", \"features.1.1.block.1.fc2.bias\", \"features.1.1.block.2.0.weight\", \"features.1.1.block.2.1.weight\", \"features.1.1.block.2.1.bias\", \"features.1.1.block.2.1.running_mean\", \"features.1.1.block.2.1.running_var\", \"features.1.1.block.2.1.num_batches_tracked\", \"features.2.0.block.0.0.weight\", \"features.2.0.block.0.1.weight\", \"features.2.0.block.0.1.bias\", \"features.2.0.block.0.1.running_mean\", \"features.2.0.block.0.1.running_var\", \"features.2.0.block.0.1.num_batches_tracked\", \"features.2.0.block.1.0.weight\", \"features.2.0.block.1.1.weight\", \"features.2.0.block.1.1.bias\", \"features.2.0.block.1.1.running_mean\", \"features.2.0.block.1.1.running_var\", \"features.2.0.block.1.1.num_batches_tracked\", \"features.2.0.block.2.fc1.weight\", \"features.2.0.block.2.fc1.bias\", \"features.2.0.block.2.fc2.weight\", \"features.2.0.block.2.fc2.bias\", \"features.2.0.block.3.0.weight\", \"features.2.0.block.3.1.weight\", \"features.2.0.block.3.1.bias\", \"features.2.0.block.3.1.running_mean\", \"features.2.0.block.3.1.running_var\", \"features.2.0.block.3.1.num_batches_tracked\", \"features.2.1.block.0.0.weight\", \"features.2.1.block.0.1.weight\", \"features.2.1.block.0.1.bias\", \"features.2.1.block.0.1.running_mean\", \"features.2.1.block.0.1.running_var\", \"features.2.1.block.0.1.num_batches_tracked\", \"features.2.1.block.1.0.weight\", \"features.2.1.block.1.1.weight\", \"features.2.1.block.1.1.bias\", \"features.2.1.block.1.1.running_mean\", \"features.2.1.block.1.1.running_var\", \"features.2.1.block.1.1.num_batches_tracked\", \"features.2.1.block.2.fc1.weight\", \"features.2.1.block.2.fc1.bias\", \"features.2.1.block.2.fc2.weight\", \"features.2.1.block.2.fc2.bias\", \"features.2.1.block.3.0.weight\", \"features.2.1.block.3.1.weight\", \"features.2.1.block.3.1.bias\", \"features.2.1.block.3.1.running_mean\", \"features.2.1.block.3.1.running_var\", \"features.2.1.block.3.1.num_batches_tracked\", \"features.2.2.block.0.0.weight\", \"features.2.2.block.0.1.weight\", \"features.2.2.block.0.1.bias\", \"features.2.2.block.0.1.running_mean\", \"features.2.2.block.0.1.running_var\", \"features.2.2.block.0.1.num_batches_tracked\", \"features.2.2.block.1.0.weight\", \"features.2.2.block.1.1.weight\", \"features.2.2.block.1.1.bias\", \"features.2.2.block.1.1.running_mean\", \"features.2.2.block.1.1.running_var\", \"features.2.2.block.1.1.num_batches_tracked\", \"features.2.2.block.2.fc1.weight\", \"features.2.2.block.2.fc1.bias\", \"features.2.2.block.2.fc2.weight\", \"features.2.2.block.2.fc2.bias\", \"features.2.2.block.3.0.weight\", \"features.2.2.block.3.1.weight\", \"features.2.2.block.3.1.bias\", \"features.2.2.block.3.1.running_mean\", \"features.2.2.block.3.1.running_var\", \"features.2.2.block.3.1.num_batches_tracked\", \"features.3.0.block.0.0.weight\", \"features.3.0.block.0.1.weight\", \"features.3.0.block.0.1.bias\", \"features.3.0.block.0.1.running_mean\", \"features.3.0.block.0.1.running_var\", \"features.3.0.block.0.1.num_batches_tracked\", \"features.3.0.block.1.0.weight\", \"features.3.0.block.1.1.weight\", \"features.3.0.block.1.1.bias\", \"features.3.0.block.1.1.running_mean\", \"features.3.0.block.1.1.running_var\", \"features.3.0.block.1.1.num_batches_tracked\", \"features.3.0.block.2.fc1.weight\", \"features.3.0.block.2.fc1.bias\", \"features.3.0.block.2.fc2.weight\", \"features.3.0.block.2.fc2.bias\", \"features.3.0.block.3.0.weight\", \"features.3.0.block.3.1.weight\", \"features.3.0.block.3.1.bias\", \"features.3.0.block.3.1.running_mean\", \"features.3.0.block.3.1.running_var\", \"features.3.0.block.3.1.num_batches_tracked\", \"features.3.1.block.0.0.weight\", \"features.3.1.block.0.1.weight\", \"features.3.1.block.0.1.bias\", \"features.3.1.block.0.1.running_mean\", \"features.3.1.block.0.1.running_var\", \"features.3.1.block.0.1.num_batches_tracked\", \"features.3.1.block.1.0.weight\", \"features.3.1.block.1.1.weight\", \"features.3.1.block.1.1.bias\", \"features.3.1.block.1.1.running_mean\", \"features.3.1.block.1.1.running_var\", \"features.3.1.block.1.1.num_batches_tracked\", \"features.3.1.block.2.fc1.weight\", \"features.3.1.block.2.fc1.bias\", \"features.3.1.block.2.fc2.weight\", \"features.3.1.block.2.fc2.bias\", \"features.3.1.block.3.0.weight\", \"features.3.1.block.3.1.weight\", \"features.3.1.block.3.1.bias\", \"features.3.1.block.3.1.running_mean\", \"features.3.1.block.3.1.running_var\", \"features.3.1.block.3.1.num_batches_tracked\", \"features.3.2.block.0.0.weight\", \"features.3.2.block.0.1.weight\", \"features.3.2.block.0.1.bias\", \"features.3.2.block.0.1.running_mean\", \"features.3.2.block.0.1.running_var\", \"features.3.2.block.0.1.num_batches_tracked\", \"features.3.2.block.1.0.weight\", \"features.3.2.block.1.1.weight\", \"features.3.2.block.1.1.bias\", \"features.3.2.block.1.1.running_mean\", \"features.3.2.block.1.1.running_var\", \"features.3.2.block.1.1.num_batches_tracked\", \"features.3.2.block.2.fc1.weight\", \"features.3.2.block.2.fc1.bias\", \"features.3.2.block.2.fc2.weight\", \"features.3.2.block.2.fc2.bias\", \"features.3.2.block.3.0.weight\", \"features.3.2.block.3.1.weight\", \"features.3.2.block.3.1.bias\", \"features.3.2.block.3.1.running_mean\", \"features.3.2.block.3.1.running_var\", \"features.3.2.block.3.1.num_batches_tracked\", \"features.4.0.block.0.0.weight\", \"features.4.0.block.0.1.weight\", \"features.4.0.block.0.1.bias\", \"features.4.0.block.0.1.running_mean\", \"features.4.0.block.0.1.running_var\", \"features.4.0.block.0.1.num_batches_tracked\", \"features.4.0.block.1.0.weight\", \"features.4.0.block.1.1.weight\", \"features.4.0.block.1.1.bias\", \"features.4.0.block.1.1.running_mean\", \"features.4.0.block.1.1.running_var\", \"features.4.0.block.1.1.num_batches_tracked\", \"features.4.0.block.2.fc1.weight\", \"features.4.0.block.2.fc1.bias\", \"features.4.0.block.2.fc2.weight\", \"features.4.0.block.2.fc2.bias\", \"features.4.0.block.3.0.weight\", \"features.4.0.block.3.1.weight\", \"features.4.0.block.3.1.bias\", \"features.4.0.block.3.1.running_mean\", \"features.4.0.block.3.1.running_var\", \"features.4.0.block.3.1.num_batches_tracked\", \"features.4.1.block.0.0.weight\", \"features.4.1.block.0.1.weight\", \"features.4.1.block.0.1.bias\", \"features.4.1.block.0.1.running_mean\", \"features.4.1.block.0.1.running_var\", \"features.4.1.block.0.1.num_batches_tracked\", \"features.4.1.block.1.0.weight\", \"features.4.1.block.1.1.weight\", \"features.4.1.block.1.1.bias\", \"features.4.1.block.1.1.running_mean\", \"features.4.1.block.1.1.running_var\", \"features.4.1.block.1.1.num_batches_tracked\", \"features.4.1.block.2.fc1.weight\", \"features.4.1.block.2.fc1.bias\", \"features.4.1.block.2.fc2.weight\", \"features.4.1.block.2.fc2.bias\", \"features.4.1.block.3.0.weight\", \"features.4.1.block.3.1.weight\", \"features.4.1.block.3.1.bias\", \"features.4.1.block.3.1.running_mean\", \"features.4.1.block.3.1.running_var\", \"features.4.1.block.3.1.num_batches_tracked\", \"features.4.2.block.0.0.weight\", \"features.4.2.block.0.1.weight\", \"features.4.2.block.0.1.bias\", \"features.4.2.block.0.1.running_mean\", \"features.4.2.block.0.1.running_var\", \"features.4.2.block.0.1.num_batches_tracked\", \"features.4.2.block.1.0.weight\", \"features.4.2.block.1.1.weight\", \"features.4.2.block.1.1.bias\", \"features.4.2.block.1.1.running_mean\", \"features.4.2.block.1.1.running_var\", \"features.4.2.block.1.1.num_batches_tracked\", \"features.4.2.block.2.fc1.weight\", \"features.4.2.block.2.fc1.bias\", \"features.4.2.block.2.fc2.weight\", \"features.4.2.block.2.fc2.bias\", \"features.4.2.block.3.0.weight\", \"features.4.2.block.3.1.weight\", \"features.4.2.block.3.1.bias\", \"features.4.2.block.3.1.running_mean\", \"features.4.2.block.3.1.running_var\", \"features.4.2.block.3.1.num_batches_tracked\", \"features.4.3.block.0.0.weight\", \"features.4.3.block.0.1.weight\", \"features.4.3.block.0.1.bias\", \"features.4.3.block.0.1.running_mean\", \"features.4.3.block.0.1.running_var\", \"features.4.3.block.0.1.num_batches_tracked\", \"features.4.3.block.1.0.weight\", \"features.4.3.block.1.1.weight\", \"features.4.3.block.1.1.bias\", \"features.4.3.block.1.1.running_mean\", \"features.4.3.block.1.1.running_var\", \"features.4.3.block.1.1.num_batches_tracked\", \"features.4.3.block.2.fc1.weight\", \"features.4.3.block.2.fc1.bias\", \"features.4.3.block.2.fc2.weight\", \"features.4.3.block.2.fc2.bias\", \"features.4.3.block.3.0.weight\", \"features.4.3.block.3.1.weight\", \"features.4.3.block.3.1.bias\", \"features.4.3.block.3.1.running_mean\", \"features.4.3.block.3.1.running_var\", \"features.4.3.block.3.1.num_batches_tracked\", \"features.5.0.block.0.0.weight\", \"features.5.0.block.0.1.weight\", \"features.5.0.block.0.1.bias\", \"features.5.0.block.0.1.running_mean\", \"features.5.0.block.0.1.running_var\", \"features.5.0.block.0.1.num_batches_tracked\", \"features.5.0.block.1.0.weight\", \"features.5.0.block.1.1.weight\", \"features.5.0.block.1.1.bias\", \"features.5.0.block.1.1.running_mean\", \"features.5.0.block.1.1.running_var\", \"features.5.0.block.1.1.num_batches_tracked\", \"features.5.0.block.2.fc1.weight\", \"features.5.0.block.2.fc1.bias\", \"features.5.0.block.2.fc2.weight\", \"features.5.0.block.2.fc2.bias\", \"features.5.0.block.3.0.weight\", \"features.5.0.block.3.1.weight\", \"features.5.0.block.3.1.bias\", \"features.5.0.block.3.1.running_mean\", \"features.5.0.block.3.1.running_var\", \"features.5.0.block.3.1.num_batches_tracked\", \"features.5.1.block.0.0.weight\", \"features.5.1.block.0.1.weight\", \"features.5.1.block.0.1.bias\", \"features.5.1.block.0.1.running_mean\", \"features.5.1.block.0.1.running_var\", \"features.5.1.block.0.1.num_batches_tracked\", \"features.5.1.block.1.0.weight\", \"features.5.1.block.1.1.weight\", \"features.5.1.block.1.1.bias\", \"features.5.1.block.1.1.running_mean\", \"features.5.1.block.1.1.running_var\", \"features.5.1.block.1.1.num_batches_tracked\", \"features.5.1.block.2.fc1.weight\", \"features.5.1.block.2.fc1.bias\", \"features.5.1.block.2.fc2.weight\", \"features.5.1.block.2.fc2.bias\", \"features.5.1.block.3.0.weight\", \"features.5.1.block.3.1.weight\", \"features.5.1.block.3.1.bias\", \"features.5.1.block.3.1.running_mean\", \"features.5.1.block.3.1.running_var\", \"features.5.1.block.3.1.num_batches_tracked\", \"features.5.2.block.0.0.weight\", \"features.5.2.block.0.1.weight\", \"features.5.2.block.0.1.bias\", \"features.5.2.block.0.1.running_mean\", \"features.5.2.block.0.1.running_var\", \"features.5.2.block.0.1.num_batches_tracked\", \"features.5.2.block.1.0.weight\", \"features.5.2.block.1.1.weight\", \"features.5.2.block.1.1.bias\", \"features.5.2.block.1.1.running_mean\", \"features.5.2.block.1.1.running_var\", \"features.5.2.block.1.1.num_batches_tracked\", \"features.5.2.block.2.fc1.weight\", \"features.5.2.block.2.fc1.bias\", \"features.5.2.block.2.fc2.weight\", \"features.5.2.block.2.fc2.bias\", \"features.5.2.block.3.0.weight\", \"features.5.2.block.3.1.weight\", \"features.5.2.block.3.1.bias\", \"features.5.2.block.3.1.running_mean\", \"features.5.2.block.3.1.running_var\", \"features.5.2.block.3.1.num_batches_tracked\", \"features.5.3.block.0.0.weight\", \"features.5.3.block.0.1.weight\", \"features.5.3.block.0.1.bias\", \"features.5.3.block.0.1.running_mean\", \"features.5.3.block.0.1.running_var\", \"features.5.3.block.0.1.num_batches_tracked\", \"features.5.3.block.1.0.weight\", \"features.5.3.block.1.1.weight\", \"features.5.3.block.1.1.bias\", \"features.5.3.block.1.1.running_mean\", \"features.5.3.block.1.1.running_var\", \"features.5.3.block.1.1.num_batches_tracked\", \"features.5.3.block.2.fc1.weight\", \"features.5.3.block.2.fc1.bias\", \"features.5.3.block.2.fc2.weight\", \"features.5.3.block.2.fc2.bias\", \"features.5.3.block.3.0.weight\", \"features.5.3.block.3.1.weight\", \"features.5.3.block.3.1.bias\", \"features.5.3.block.3.1.running_mean\", \"features.5.3.block.3.1.running_var\", \"features.5.3.block.3.1.num_batches_tracked\", \"features.6.0.block.0.0.weight\", \"features.6.0.block.0.1.weight\", \"features.6.0.block.0.1.bias\", \"features.6.0.block.0.1.running_mean\", \"features.6.0.block.0.1.running_var\", \"features.6.0.block.0.1.num_batches_tracked\", \"features.6.0.block.1.0.weight\", \"features.6.0.block.1.1.weight\", \"features.6.0.block.1.1.bias\", \"features.6.0.block.1.1.running_mean\", \"features.6.0.block.1.1.running_var\", \"features.6.0.block.1.1.num_batches_tracked\", \"features.6.0.block.2.fc1.weight\", \"features.6.0.block.2.fc1.bias\", \"features.6.0.block.2.fc2.weight\", \"features.6.0.block.2.fc2.bias\", \"features.6.0.block.3.0.weight\", \"features.6.0.block.3.1.weight\", \"features.6.0.block.3.1.bias\", \"features.6.0.block.3.1.running_mean\", \"features.6.0.block.3.1.running_var\", \"features.6.0.block.3.1.num_batches_tracked\", \"features.6.1.block.0.0.weight\", \"features.6.1.block.0.1.weight\", \"features.6.1.block.0.1.bias\", \"features.6.1.block.0.1.running_mean\", \"features.6.1.block.0.1.running_var\", \"features.6.1.block.0.1.num_batches_tracked\", \"features.6.1.block.1.0.weight\", \"features.6.1.block.1.1.weight\", \"features.6.1.block.1.1.bias\", \"features.6.1.block.1.1.running_mean\", \"features.6.1.block.1.1.running_var\", \"features.6.1.block.1.1.num_batches_tracked\", \"features.6.1.block.2.fc1.weight\", \"features.6.1.block.2.fc1.bias\", \"features.6.1.block.2.fc2.weight\", \"features.6.1.block.2.fc2.bias\", \"features.6.1.block.3.0.weight\", \"features.6.1.block.3.1.weight\", \"features.6.1.block.3.1.bias\", \"features.6.1.block.3.1.running_mean\", \"features.6.1.block.3.1.running_var\", \"features.6.1.block.3.1.num_batches_tracked\", \"features.6.2.block.0.0.weight\", \"features.6.2.block.0.1.weight\", \"features.6.2.block.0.1.bias\", \"features.6.2.block.0.1.running_mean\", \"features.6.2.block.0.1.running_var\", \"features.6.2.block.0.1.num_batches_tracked\", \"features.6.2.block.1.0.weight\", \"features.6.2.block.1.1.weight\", \"features.6.2.block.1.1.bias\", \"features.6.2.block.1.1.running_mean\", \"features.6.2.block.1.1.running_var\", \"features.6.2.block.1.1.num_batches_tracked\", \"features.6.2.block.2.fc1.weight\", \"features.6.2.block.2.fc1.bias\", \"features.6.2.block.2.fc2.weight\", \"features.6.2.block.2.fc2.bias\", \"features.6.2.block.3.0.weight\", \"features.6.2.block.3.1.weight\", \"features.6.2.block.3.1.bias\", \"features.6.2.block.3.1.running_mean\", \"features.6.2.block.3.1.running_var\", \"features.6.2.block.3.1.num_batches_tracked\", \"features.6.3.block.0.0.weight\", \"features.6.3.block.0.1.weight\", \"features.6.3.block.0.1.bias\", \"features.6.3.block.0.1.running_mean\", \"features.6.3.block.0.1.running_var\", \"features.6.3.block.0.1.num_batches_tracked\", \"features.6.3.block.1.0.weight\", \"features.6.3.block.1.1.weight\", \"features.6.3.block.1.1.bias\", \"features.6.3.block.1.1.running_mean\", \"features.6.3.block.1.1.running_var\", \"features.6.3.block.1.1.num_batches_tracked\", \"features.6.3.block.2.fc1.weight\", \"features.6.3.block.2.fc1.bias\", \"features.6.3.block.2.fc2.weight\", \"features.6.3.block.2.fc2.bias\", \"features.6.3.block.3.0.weight\", \"features.6.3.block.3.1.weight\", \"features.6.3.block.3.1.bias\", \"features.6.3.block.3.1.running_mean\", \"features.6.3.block.3.1.running_var\", \"features.6.3.block.3.1.num_batches_tracked\", \"features.6.4.block.0.0.weight\", \"features.6.4.block.0.1.weight\", \"features.6.4.block.0.1.bias\", \"features.6.4.block.0.1.running_mean\", \"features.6.4.block.0.1.running_var\", \"features.6.4.block.0.1.num_batches_tracked\", \"features.6.4.block.1.0.weight\", \"features.6.4.block.1.1.weight\", \"features.6.4.block.1.1.bias\", \"features.6.4.block.1.1.running_mean\", \"features.6.4.block.1.1.running_var\", \"features.6.4.block.1.1.num_batches_tracked\", \"features.6.4.block.2.fc1.weight\", \"features.6.4.block.2.fc1.bias\", \"features.6.4.block.2.fc2.weight\", \"features.6.4.block.2.fc2.bias\", \"features.6.4.block.3.0.weight\", \"features.6.4.block.3.1.weight\", \"features.6.4.block.3.1.bias\", \"features.6.4.block.3.1.running_mean\", \"features.6.4.block.3.1.running_var\", \"features.6.4.block.3.1.num_batches_tracked\", \"features.7.0.block.0.0.weight\", \"features.7.0.block.0.1.weight\", \"features.7.0.block.0.1.bias\", \"features.7.0.block.0.1.running_mean\", \"features.7.0.block.0.1.running_var\", \"features.7.0.block.0.1.num_batches_tracked\", \"features.7.0.block.1.0.weight\", \"features.7.0.block.1.1.weight\", \"features.7.0.block.1.1.bias\", \"features.7.0.block.1.1.running_mean\", \"features.7.0.block.1.1.running_var\", \"features.7.0.block.1.1.num_batches_tracked\", \"features.7.0.block.2.fc1.weight\", \"features.7.0.block.2.fc1.bias\", \"features.7.0.block.2.fc2.weight\", \"features.7.0.block.2.fc2.bias\", \"features.7.0.block.3.0.weight\", \"features.7.0.block.3.1.weight\", \"features.7.0.block.3.1.bias\", \"features.7.0.block.3.1.running_mean\", \"features.7.0.block.3.1.running_var\", \"features.7.0.block.3.1.num_batches_tracked\", \"features.7.1.block.0.0.weight\", \"features.7.1.block.0.1.weight\", \"features.7.1.block.0.1.bias\", \"features.7.1.block.0.1.running_mean\", \"features.7.1.block.0.1.running_var\", \"features.7.1.block.0.1.num_batches_tracked\", \"features.7.1.block.1.0.weight\", \"features.7.1.block.1.1.weight\", \"features.7.1.block.1.1.bias\", \"features.7.1.block.1.1.running_mean\", \"features.7.1.block.1.1.running_var\", \"features.7.1.block.1.1.num_batches_tracked\", \"features.7.1.block.2.fc1.weight\", \"features.7.1.block.2.fc1.bias\", \"features.7.1.block.2.fc2.weight\", \"features.7.1.block.2.fc2.bias\", \"features.7.1.block.3.0.weight\", \"features.7.1.block.3.1.weight\", \"features.7.1.block.3.1.bias\", \"features.7.1.block.3.1.running_mean\", \"features.7.1.block.3.1.running_var\", \"features.7.1.block.3.1.num_batches_tracked\", \"features.8.0.weight\", \"features.8.1.weight\", \"features.8.1.bias\", \"features.8.1.running_mean\", \"features.8.1.running_var\", \"features.8.1.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model2 = FusionModel(meta_input_dim=\u001b[32m19\u001b[39m, num_classes=\u001b[32m7\u001b[39m)\n\u001b[32m      2\u001b[39m model2 = train_cnn_first(model2, train_loader, val_loader, epochs=\u001b[32m20\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model2 = \u001b[43mtrain_fusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m results = test_model(model2, test_loader, class_names)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mtrain_fusion_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, device)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Restore best weights\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_model_weights:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sabad\\OneDrive\\Desktop\\APS360\\HAM10K Dataset\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2573\u001b[39m         error_msgs.insert(\n\u001b[32m   2574\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2575\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2576\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2577\u001b[39m             ),\n\u001b[32m   2578\u001b[39m         )\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2583\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2584\u001b[39m         )\n\u001b[32m   2585\u001b[39m     )\n\u001b[32m   2586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for FusionModel:\n\tMissing key(s) in state_dict: \"image_model.features.0.0.weight\", \"image_model.features.0.1.weight\", \"image_model.features.0.1.bias\", \"image_model.features.0.1.running_mean\", \"image_model.features.0.1.running_var\", \"image_model.features.1.0.block.0.0.weight\", \"image_model.features.1.0.block.0.1.weight\", \"image_model.features.1.0.block.0.1.bias\", \"image_model.features.1.0.block.0.1.running_mean\", \"image_model.features.1.0.block.0.1.running_var\", \"image_model.features.1.0.block.1.fc1.weight\", \"image_model.features.1.0.block.1.fc1.bias\", \"image_model.features.1.0.block.1.fc2.weight\", \"image_model.features.1.0.block.1.fc2.bias\", \"image_model.features.1.0.block.2.0.weight\", \"image_model.features.1.0.block.2.1.weight\", \"image_model.features.1.0.block.2.1.bias\", \"image_model.features.1.0.block.2.1.running_mean\", \"image_model.features.1.0.block.2.1.running_var\", \"image_model.features.1.1.block.0.0.weight\", \"image_model.features.1.1.block.0.1.weight\", \"image_model.features.1.1.block.0.1.bias\", \"image_model.features.1.1.block.0.1.running_mean\", \"image_model.features.1.1.block.0.1.running_var\", \"image_model.features.1.1.block.1.fc1.weight\", \"image_model.features.1.1.block.1.fc1.bias\", \"image_model.features.1.1.block.1.fc2.weight\", \"image_model.features.1.1.block.1.fc2.bias\", \"image_model.features.1.1.block.2.0.weight\", \"image_model.features.1.1.block.2.1.weight\", \"image_model.features.1.1.block.2.1.bias\", \"image_model.features.1.1.block.2.1.running_mean\", \"image_model.features.1.1.block.2.1.running_var\", \"image_model.features.2.0.block.0.0.weight\", \"image_model.features.2.0.block.0.1.weight\", \"image_model.features.2.0.block.0.1.bias\", \"image_model.features.2.0.block.0.1.running_mean\", \"image_model.features.2.0.block.0.1.running_var\", \"image_model.features.2.0.block.1.0.weight\", \"image_model.features.2.0.block.1.1.weight\", \"image_model.features.2.0.block.1.1.bias\", \"image_model.features.2.0.block.1.1.running_mean\", \"image_model.features.2.0.block.1.1.running_var\", \"image_model.features.2.0.block.2.fc1.weight\", \"image_model.features.2.0.block.2.fc1.bias\", \"image_model.features.2.0.block.2.fc2.weight\", \"image_model.features.2.0.block.2.fc2.bias\", \"image_model.features.2.0.block.3.0.weight\", \"image_model.features.2.0.block.3.1.weight\", \"image_model.features.2.0.block.3.1.bias\", \"image_model.features.2.0.block.3.1.running_mean\", \"image_model.features.2.0.block.3.1.running_var\", \"image_model.features.2.1.block.0.0.weight\", \"image_model.features.2.1.block.0.1.weight\", \"image_model.features.2.1.block.0.1.bias\", \"image_model.features.2.1.block.0.1.running_mean\", \"image_model.features.2.1.block.0.1.running_var\", \"image_model.features.2.1.block.1.0.weight\", \"image_model.features.2.1.block.1.1.weight\", \"image_model.features.2.1.block.1.1.bias\", \"image_model.features.2.1.block.1.1.running_mean\", \"image_model.features.2.1.block.1.1.running_var\", \"image_model.features.2.1.block.2.fc1.weight\", \"image_model.features.2.1.block.2.fc1.bias\", \"image_model.features.2.1.block.2.fc2.weight\", \"image_model.features.2.1.block.2.fc2.bias\", \"image_model.features.2.1.block.3.0.weight\", \"image_model.features.2.1.block.3.1.weight\", \"image_model.features.2.1.block.3.1.bias\", \"image_model.features.2.1.block.3.1.running_mean\", \"image_model.features.2.1.block.3.1.running_var\", \"image_model.features.2.2.block.0.0.weight\", \"image_model.features.2.2.block.0.1.weight\", \"image_model.features.2.2.block.0.1.bias\", \"image_model.features.2.2.block.0.1.running_mean\", \"image_model.features.2.2.block.0.1.running_var\", \"image_model.features.2.2.block.1.0.weight\", \"image_model.features.2.2.block.1.1.weight\", \"image_model.features.2.2.block.1.1.bias\", \"image_model.features.2.2.block.1.1.running_mean\", \"image_model.features.2.2.block.1.1.running_var\", \"image_model.features.2.2.block.2.fc1.weight\", \"image_model.features.2.2.block.2.fc1.bias\", \"image_model.features.2.2.block.2.fc2.weight\", \"image_model.features.2.2.block.2.fc2.bias\", \"image_model.features.2.2.block.3.0.weight\", \"image_model.features.2.2.block.3.1.weight\", \"image_model.features.2.2.block.3.1.bias\", \"image_model.features.2.2.block.3.1.running_mean\", \"image_model.features.2.2.block.3.1.running_var\", \"image_model.features.3.0.block.0.0.weight\", \"image_model.features.3.0.block.0.1.weight\", \"image_model.features.3.0.block.0.1.bias\", \"image_model.features.3.0.block.0.1.running_mean\", \"image_model.features.3.0.block.0.1.running_var\", \"image_model.features.3.0.block.1.0.weight\", \"image_model.features.3.0.block.1.1.weight\", \"image_model.features.3.0.block.1.1.bias\", \"image_model.features.3.0.block.1.1.running_mean\", \"image_model.features.3.0.block.1.1.running_var\", \"image_model.features.3.0.block.2.fc1.weight\", \"image_model.features.3.0.block.2.fc1.bias\", \"image_model.features.3.0.block.2.fc2.weight\", \"image_model.features.3.0.block.2.fc2.bias\", \"image_model.features.3.0.block.3.0.weight\", \"image_model.features.3.0.block.3.1.weight\", \"image_model.features.3.0.block.3.1.bias\", \"image_model.features.3.0.block.3.1.running_mean\", \"image_model.features.3.0.block.3.1.running_var\", \"image_model.features.3.1.block.0.0.weight\", \"image_model.features.3.1.block.0.1.weight\", \"image_model.features.3.1.block.0.1.bias\", \"image_model.features.3.1.block.0.1.running_mean\", \"image_model.features.3.1.block.0.1.running_var\", \"image_model.features.3.1.block.1.0.weight\", \"image_model.features.3.1.block.1.1.weight\", \"image_model.features.3.1.block.1.1.bias\", \"image_model.features.3.1.block.1.1.running_mean\", \"image_model.features.3.1.block.1.1.running_var\", \"image_model.features.3.1.block.2.fc1.weight\", \"image_model.features.3.1.block.2.fc1.bias\", \"image_model.features.3.1.block.2.fc2.weight\", \"image_model.features.3.1.block.2.fc2.bias\", \"image_model.features.3.1.block.3.0.weight\", \"image_model.features.3.1.block.3.1.weight\", \"image_model.features.3.1.block.3.1.bias\", \"image_model.features.3.1.block.3.1.running_mean\", \"image_model.features.3.1.block.3.1.running_var\", \"image_model.features.3.2.block.0.0.weight\", \"image_model.features.3.2.block.0.1.weight\", \"image_model.features.3.2.block.0.1.bias\", \"image_model.features.3.2.block.0.1.running_mean\", \"image_model.features.3.2.block.0.1.running_var\", \"image_model.features.3.2.block.1.0.weight\", \"image_model.features.3.2.block.1.1.weight\", \"image_model.features.3.2.block.1.1.bias\", \"image_model.features.3.2.block.1.1.running_mean\", \"image_model.features.3.2.block.1.1.running_var\", \"image_model.features.3.2.block.2.fc1.weight\", \"image_model.features.3.2.block.2.fc1.bias\", \"image_model.features.3.2.block.2.fc2.weight\", \"image_model.features.3.2.block.2.fc2.bias\", \"image_model.features.3.2.block.3.0.weight\", \"image_model.features.3.2.block.3.1.weight\", \"image_model.features.3.2.block.3.1.bias\", \"image_model.features.3.2.block.3.1.running_mean\", \"image_model.features.3.2.block.3.1.running_var\", \"image_model.features.4.0.block.0.0.weight\", \"image_model.features.4.0.block.0.1.weight\", \"image_model.features.4.0.block.0.1.bias\", \"image_model.features.4.0.block.0.1.running_mean\", \"image_model.features.4.0.block.0.1.running_var\", \"image_model.features.4.0.block.1.0.weight\", \"image_model.features.4.0.block.1.1.weight\", \"image_model.features.4.0.block.1.1.bias\", \"image_model.features.4.0.block.1.1.running_mean\", \"image_model.features.4.0.block.1.1.running_var\", \"image_model.features.4.0.block.2.fc1.weight\", \"image_model.features.4.0.block.2.fc1.bias\", \"image_model.features.4.0.block.2.fc2.weight\", \"image_model.features.4.0.block.2.fc2.bias\", \"image_model.features.4.0.block.3.0.weight\", \"image_model.features.4.0.block.3.1.weight\", \"image_model.features.4.0.block.3.1.bias\", \"image_model.features.4.0.block.3.1.running_mean\", \"image_model.features.4.0.block.3.1.running_var\", \"image_model.features.4.1.block.0.0.weight\", \"image_model.features.4.1.block.0.1.weight\", \"image_model.features.4.1.block.0.1.bias\", \"image_model.features.4.1.block.0.1.running_mean\", \"image_model.features.4.1.block.0.1.running_var\", \"image_model.features.4.1.block.1.0.weight\", \"image_model.features.4.1.block.1.1.weight\", \"image_model.features.4.1.block.1.1.bias\", \"image_model.features.4.1.block.1.1.running_mean\", \"image_model.features.4.1.block.1.1.running_var\", \"image_model.features.4.1.block.2.fc1.weight\", \"image_model.features.4.1.block.2.fc1.bias\", \"image_model.features.4.1.block.2.fc2.weight\", \"image_model.features.4.1.block.2.fc2.bias\", \"image_model.features.4.1.block.3.0.weight\", \"image_model.features.4.1.block.3.1.weight\", \"image_model.features.4.1.block.3.1.bias\", \"image_model.features.4.1.block.3.1.running_mean\", \"image_model.features.4.1.block.3.1.running_var\", \"image_model.features.4.2.block.0.0.weight\", \"image_model.features.4.2.block.0.1.weight\", \"image_model.features.4.2.block.0.1.bias\", \"image_model.features.4.2.block.0.1.running_mean\", \"image_model.features.4.2.block.0.1.running_var\", \"image_model.features.4.2.block.1.0.weight\", \"image_model.features.4.2.block.1.1.weight\", \"image_model.features.4.2.block.1.1.bias\", \"image_model.features.4.2.block.1.1.running_mean\", \"image_model.features.4.2.block.1.1.running_var\", \"image_model.features.4.2.block.2.fc1.weight\", \"image_model.features.4.2.block.2.fc1.bias\", \"image_model.features.4.2.block.2.fc2.weight\", \"image_model.features.4.2.block.2.fc2.bias\", \"image_model.features.4.2.block.3.0.weight\", \"image_model.features.4.2.block.3.1.weight\", \"image_model.features.4.2.block.3.1.bias\", \"image_model.features.4.2.block.3.1.running_mean\", \"image_model.features.4.2.block.3.1.running_var\", \"image_model.features.4.3.block.0.0.weight\", \"image_model.features.4.3.block.0.1.weight\", \"image_model.features.4.3.block.0.1.bias\", \"image_model.features.4.3.block.0.1.running_mean\", \"image_model.features.4.3.block.0.1.running_var\", \"image_model.features.4.3.block.1.0.weight\", \"image_model.features.4.3.block.1.1.weight\", \"image_model.features.4.3.block.1.1.bias\", \"image_model.features.4.3.block.1.1.running_mean\", \"image_model.features.4.3.block.1.1.running_var\", \"image_model.features.4.3.block.2.fc1.weight\", \"image_model.features.4.3.block.2.fc1.bias\", \"image_model.features.4.3.block.2.fc2.weight\", \"image_model.features.4.3.block.2.fc2.bias\", \"image_model.features.4.3.block.3.0.weight\", \"image_model.features.4.3.block.3.1.weight\", \"image_model.features.4.3.block.3.1.bias\", \"image_model.features.4.3.block.3.1.running_mean\", \"image_model.features.4.3.block.3.1.running_var\", \"image_model.features.5.0.block.0.0.weight\", \"image_model.features.5.0.block.0.1.weight\", \"image_model.features.5.0.block.0.1.bias\", \"image_model.features.5.0.block.0.1.running_mean\", \"image_model.features.5.0.block.0.1.running_var\", \"image_model.features.5.0.block.1.0.weight\", \"image_model.features.5.0.block.1.1.weight\", \"image_model.features.5.0.block.1.1.bias\", \"image_model.features.5.0.block.1.1.running_mean\", \"image_model.features.5.0.block.1.1.running_var\", \"image_model.features.5.0.block.2.fc1.weight\", \"image_model.features.5.0.block.2.fc1.bias\", \"image_model.features.5.0.block.2.fc2.weight\", \"image_model.features.5.0.block.2.fc2.bias\", \"image_model.features.5.0.block.3.0.weight\", \"image_model.features.5.0.block.3.1.weight\", \"image_model.features.5.0.block.3.1.bias\", \"image_model.features.5.0.block.3.1.running_mean\", \"image_model.features.5.0.block.3.1.running_var\", \"image_model.features.5.1.block.0.0.weight\", \"image_model.features.5.1.block.0.1.weight\", \"image_model.features.5.1.block.0.1.bias\", \"image_model.features.5.1.block.0.1.running_mean\", \"image_model.features.5.1.block.0.1.running_var\", \"image_model.features.5.1.block.1.0.weight\", \"image_model.features.5.1.block.1.1.weight\", \"image_model.features.5.1.block.1.1.bias\", \"image_model.features.5.1.block.1.1.running_mean\", \"image_model.features.5.1.block.1.1.running_var\", \"image_model.features.5.1.block.2.fc1.weight\", \"image_model.features.5.1.block.2.fc1.bias\", \"image_model.features.5.1.block.2.fc2.weight\", \"image_model.features.5.1.block.2.fc2.bias\", \"image_model.features.5.1.block.3.0.weight\", \"image_model.features.5.1.block.3.1.weight\", \"image_model.features.5.1.block.3.1.bias\", \"image_model.features.5.1.block.3.1.running_mean\", \"image_model.features.5.1.block.3.1.running_var\", \"image_model.features.5.2.block.0.0.weight\", \"image_model.features.5.2.block.0.1.weight\", \"image_model.features.5.2.block.0.1.bias\", \"image_model.features.5.2.block.0.1.running_mean\", \"image_model.features.5.2.block.0.1.running_var\", \"image_model.features.5.2.block.1.0.weight\", \"image_model.features.5.2.block.1.1.weight\", \"image_model.features.5.2.block.1.1.bias\", \"image_model.features.5.2.block.1.1.running_mean\", \"image_model.features.5.2.block.1.1.running_var\", \"image_model.features.5.2.block.2.fc1.weight\", \"image_model.features.5.2.block.2.fc1.bias\", \"image_model.features.5.2.block.2.fc2.weight\", \"image_model.features.5.2.block.2.fc2.bias\", \"image_model.features.5.2.block.3.0.weight\", \"image_model.features.5.2.block.3.1.weight\", \"image_model.features.5.2.block.3.1.bias\", \"image_model.features.5.2.block.3.1.running_mean\", \"image_model.features.5.2.block.3.1.running_var\", \"image_model.features.5.3.block.0.0.weight\", \"image_model.features.5.3.block.0.1.weight\", \"image_model.features.5.3.block.0.1.bias\", \"image_model.features.5.3.block.0.1.running_mean\", \"image_model.features.5.3.block.0.1.running_var\", \"image_model.features.5.3.block.1.0.weight\", \"image_model.features.5.3.block.1.1.weight\", \"image_model.features.5.3.block.1.1.bias\", \"image_model.features.5.3.block.1.1.running_mean\", \"image_model.features.5.3.block.1.1.running_var\", \"image_model.features.5.3.block.2.fc1.weight\", \"image_model.features.5.3.block.2.fc1.bias\", \"image_model.features.5.3.block.2.fc2.weight\", \"image_model.features.5.3.block.2.fc2.bias\", \"image_model.features.5.3.block.3.0.weight\", \"image_model.features.5.3.block.3.1.weight\", \"image_model.features.5.3.block.3.1.bias\", \"image_model.features.5.3.block.3.1.running_mean\", \"image_model.features.5.3.block.3.1.running_var\", \"image_model.features.6.0.block.0.0.weight\", \"image_model.features.6.0.block.0.1.weight\", \"image_model.features.6.0.block.0.1.bias\", \"image_model.features.6.0.block.0.1.running_mean\", \"image_model.features.6.0.block.0.1.running_var\", \"image_model.features.6.0.block.1.0.weight\", \"image_model.features.6.0.block.1.1.weight\", \"image_model.features.6.0.block.1.1.bias\", \"image_model.features.6.0.block.1.1.running_mean\", \"image_model.features.6.0.block.1.1.running_var\", \"image_model.features.6.0.block.2.fc1.weight\", \"image_model.features.6.0.block.2.fc1.bias\", \"image_model.features.6.0.block.2.fc2.weight\", \"image_model.features.6.0.block.2.fc2.bias\", \"image_model.features.6.0.block.3.0.weight\", \"image_model.features.6.0.block.3.1.weight\", \"image_model.features.6.0.block.3.1.bias\", \"image_model.features.6.0.block.3.1.running_mean\", \"image_model.features.6.0.block.3.1.running_var\", \"image_model.features.6.1.block.0.0.weight\", \"image_model.features.6.1.block.0.1.weight\", \"image_model.features.6.1.block.0.1.bias\", \"image_model.features.6.1.block.0.1.running_mean\", \"image_model.features.6.1.block.0.1.running_var\", \"image_model.features.6.1.block.1.0.weight\", \"image_model.features.6.1.block.1.1.weight\", \"image_model.features.6.1.block.1.1.bias\", \"image_model.features.6.1.block.1.1.running_mean\", \"image_model.features.6.1.block.1.1.running_var\", \"image_model.features.6.1.block.2.fc1.weight\", \"image_model.features.6.1.block.2.fc1.bias\", \"image_model.features.6.1.block.2.fc2.weight\", \"image_model.features.6.1.block.2.fc2.bias\", \"image_model.features.6.1.block.3.0.weight\", \"image_model.features.6.1.block.3.1.weight\", \"image_model.features.6.1.block.3.1.bias\", \"image_model.features.6.1.block.3.1.running_mean\", \"image_model.features.6.1.block.3.1.running_var\", \"image_model.features.6.2.block.0.0.weight\", \"image_model.features.6.2.block.0.1.weight\", \"image_model.features.6.2.block.0.1.bias\", \"image_model.features.6.2.block.0.1.running_mean\", \"image_model.features.6.2.block.0.1.running_var\", \"image_model.features.6.2.block.1.0.weight\", \"image_model.features.6.2.block.1.1.weight\", \"image_model.features.6.2.block.1.1.bias\", \"image_model.features.6.2.block.1.1.running_mean\", \"image_model.features.6.2.block.1.1.running_var\", \"image_model.features.6.2.block.2.fc1.weight\", \"image_model.features.6.2.block.2.fc1.bias\", \"image_model.features.6.2.block.2.fc2.weight\", \"image_model.features.6.2.block.2.fc2.bias\", \"image_model.features.6.2.block.3.0.weight\", \"image_model.features.6.2.block.3.1.weight\", \"image_model.features.6.2.block.3.1.bias\", \"image_model.features.6.2.block.3.1.running_mean\", \"image_model.features.6.2.block.3.1.running_var\", \"image_model.features.6.3.block.0.0.weight\", \"image_model.features.6.3.block.0.1.weight\", \"image_model.features.6.3.block.0.1.bias\", \"image_model.features.6.3.block.0.1.running_mean\", \"image_model.features.6.3.block.0.1.running_var\", \"image_model.features.6.3.block.1.0.weight\", \"image_model.features.6.3.block.1.1.weight\", \"image_model.features.6.3.block.1.1.bias\", \"image_model.features.6.3.block.1.1.running_mean\", \"image_model.features.6.3.block.1.1.running_var\", \"image_model.features.6.3.block.2.fc1.weight\", \"image_model.features.6.3.block.2.fc1.bias\", \"image_model.features.6.3.block.2.fc2.weight\", \"image_model.features.6.3.block.2.fc2.bias\", \"image_model.features.6.3.block.3.0.weight\", \"image_model.features.6.3.block.3.1.weight\", \"image_model.features.6.3.block.3.1.bias\", \"image_model.features.6.3.block.3.1.running_mean\", \"image_model.features.6.3.block.3.1.running_var\", \"image_model.features.6.4.block.0.0.weight\", \"image_model.features.6.4.block.0.1.weight\", \"image_model.features.6.4.block.0.1.bias\", \"image_model.features.6.4.block.0.1.running_mean\", \"image_model.features.6.4.block.0.1.running_var\", \"image_model.features.6.4.block.1.0.weight\", \"image_model.features.6.4.block.1.1.weight\", \"image_model.features.6.4.block.1.1.bias\", \"image_model.features.6.4.block.1.1.running_mean\", \"image_model.features.6.4.block.1.1.running_var\", \"image_model.features.6.4.block.2.fc1.weight\", \"image_model.features.6.4.block.2.fc1.bias\", \"image_model.features.6.4.block.2.fc2.weight\", \"image_model.features.6.4.block.2.fc2.bias\", \"image_model.features.6.4.block.3.0.weight\", \"image_model.features.6.4.block.3.1.weight\", \"image_model.features.6.4.block.3.1.bias\", \"image_model.features.6.4.block.3.1.running_mean\", \"image_model.features.6.4.block.3.1.running_var\", \"image_model.features.7.0.block.0.0.weight\", \"image_model.features.7.0.block.0.1.weight\", \"image_model.features.7.0.block.0.1.bias\", \"image_model.features.7.0.block.0.1.running_mean\", \"image_model.features.7.0.block.0.1.running_var\", \"image_model.features.7.0.block.1.0.weight\", \"image_model.features.7.0.block.1.1.weight\", \"image_model.features.7.0.block.1.1.bias\", \"image_model.features.7.0.block.1.1.running_mean\", \"image_model.features.7.0.block.1.1.running_var\", \"image_model.features.7.0.block.2.fc1.weight\", \"image_model.features.7.0.block.2.fc1.bias\", \"image_model.features.7.0.block.2.fc2.weight\", \"image_model.features.7.0.block.2.fc2.bias\", \"image_model.features.7.0.block.3.0.weight\", \"image_model.features.7.0.block.3.1.weight\", \"image_model.features.7.0.block.3.1.bias\", \"image_model.features.7.0.block.3.1.running_mean\", \"image_model.features.7.0.block.3.1.running_var\", \"image_model.features.7.1.block.0.0.weight\", \"image_model.features.7.1.block.0.1.weight\", \"image_model.features.7.1.block.0.1.bias\", \"image_model.features.7.1.block.0.1.running_mean\", \"image_model.features.7.1.block.0.1.running_var\", \"image_model.features.7.1.block.1.0.weight\", \"image_model.features.7.1.block.1.1.weight\", \"image_model.features.7.1.block.1.1.bias\", \"image_model.features.7.1.block.1.1.running_mean\", \"image_model.features.7.1.block.1.1.running_var\", \"image_model.features.7.1.block.2.fc1.weight\", \"image_model.features.7.1.block.2.fc1.bias\", \"image_model.features.7.1.block.2.fc2.weight\", \"image_model.features.7.1.block.2.fc2.bias\", \"image_model.features.7.1.block.3.0.weight\", \"image_model.features.7.1.block.3.1.weight\", \"image_model.features.7.1.block.3.1.bias\", \"image_model.features.7.1.block.3.1.running_mean\", \"image_model.features.7.1.block.3.1.running_var\", \"image_model.features.8.0.weight\", \"image_model.features.8.1.weight\", \"image_model.features.8.1.bias\", \"image_model.features.8.1.running_mean\", \"image_model.features.8.1.running_var\", \"meta_net.0.weight\", \"meta_net.0.bias\", \"meta_net.1.weight\", \"meta_net.1.bias\", \"meta_net.1.running_mean\", \"meta_net.1.running_var\", \"meta_net.4.weight\", \"meta_net.4.bias\", \"meta_net.5.weight\", \"meta_net.5.bias\", \"meta_net.5.running_mean\", \"meta_net.5.running_var\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.1.weight\", \"classifier.1.bias\", \"classifier.1.running_mean\", \"classifier.1.running_var\", \"classifier.4.weight\", \"classifier.4.bias\". \n\tUnexpected key(s) in state_dict: \"features.0.0.weight\", \"features.0.1.weight\", \"features.0.1.bias\", \"features.0.1.running_mean\", \"features.0.1.running_var\", \"features.0.1.num_batches_tracked\", \"features.1.0.block.0.0.weight\", \"features.1.0.block.0.1.weight\", \"features.1.0.block.0.1.bias\", \"features.1.0.block.0.1.running_mean\", \"features.1.0.block.0.1.running_var\", \"features.1.0.block.0.1.num_batches_tracked\", \"features.1.0.block.1.fc1.weight\", \"features.1.0.block.1.fc1.bias\", \"features.1.0.block.1.fc2.weight\", \"features.1.0.block.1.fc2.bias\", \"features.1.0.block.2.0.weight\", \"features.1.0.block.2.1.weight\", \"features.1.0.block.2.1.bias\", \"features.1.0.block.2.1.running_mean\", \"features.1.0.block.2.1.running_var\", \"features.1.0.block.2.1.num_batches_tracked\", \"features.1.1.block.0.0.weight\", \"features.1.1.block.0.1.weight\", \"features.1.1.block.0.1.bias\", \"features.1.1.block.0.1.running_mean\", \"features.1.1.block.0.1.running_var\", \"features.1.1.block.0.1.num_batches_tracked\", \"features.1.1.block.1.fc1.weight\", \"features.1.1.block.1.fc1.bias\", \"features.1.1.block.1.fc2.weight\", \"features.1.1.block.1.fc2.bias\", \"features.1.1.block.2.0.weight\", \"features.1.1.block.2.1.weight\", \"features.1.1.block.2.1.bias\", \"features.1.1.block.2.1.running_mean\", \"features.1.1.block.2.1.running_var\", \"features.1.1.block.2.1.num_batches_tracked\", \"features.2.0.block.0.0.weight\", \"features.2.0.block.0.1.weight\", \"features.2.0.block.0.1.bias\", \"features.2.0.block.0.1.running_mean\", \"features.2.0.block.0.1.running_var\", \"features.2.0.block.0.1.num_batches_tracked\", \"features.2.0.block.1.0.weight\", \"features.2.0.block.1.1.weight\", \"features.2.0.block.1.1.bias\", \"features.2.0.block.1.1.running_mean\", \"features.2.0.block.1.1.running_var\", \"features.2.0.block.1.1.num_batches_tracked\", \"features.2.0.block.2.fc1.weight\", \"features.2.0.block.2.fc1.bias\", \"features.2.0.block.2.fc2.weight\", \"features.2.0.block.2.fc2.bias\", \"features.2.0.block.3.0.weight\", \"features.2.0.block.3.1.weight\", \"features.2.0.block.3.1.bias\", \"features.2.0.block.3.1.running_mean\", \"features.2.0.block.3.1.running_var\", \"features.2.0.block.3.1.num_batches_tracked\", \"features.2.1.block.0.0.weight\", \"features.2.1.block.0.1.weight\", \"features.2.1.block.0.1.bias\", \"features.2.1.block.0.1.running_mean\", \"features.2.1.block.0.1.running_var\", \"features.2.1.block.0.1.num_batches_tracked\", \"features.2.1.block.1.0.weight\", \"features.2.1.block.1.1.weight\", \"features.2.1.block.1.1.bias\", \"features.2.1.block.1.1.running_mean\", \"features.2.1.block.1.1.running_var\", \"features.2.1.block.1.1.num_batches_tracked\", \"features.2.1.block.2.fc1.weight\", \"features.2.1.block.2.fc1.bias\", \"features.2.1.block.2.fc2.weight\", \"features.2.1.block.2.fc2.bias\", \"features.2.1.block.3.0.weight\", \"features.2.1.block.3.1.weight\", \"features.2.1.block.3.1.bias\", \"features.2.1.block.3.1.running_mean\", \"features.2.1.block.3.1.running_var\", \"features.2.1.block.3.1.num_batches_tracked\", \"features.2.2.block.0.0.weight\", \"features.2.2.block.0.1.weight\", \"features.2.2.block.0.1.bias\", \"features.2.2.block.0.1.running_mean\", \"features.2.2.block.0.1.running_var\", \"features.2.2.block.0.1.num_batches_tracked\", \"features.2.2.block.1.0.weight\", \"features.2.2.block.1.1.weight\", \"features.2.2.block.1.1.bias\", \"features.2.2.block.1.1.running_mean\", \"features.2.2.block.1.1.running_var\", \"features.2.2.block.1.1.num_batches_tracked\", \"features.2.2.block.2.fc1.weight\", \"features.2.2.block.2.fc1.bias\", \"features.2.2.block.2.fc2.weight\", \"features.2.2.block.2.fc2.bias\", \"features.2.2.block.3.0.weight\", \"features.2.2.block.3.1.weight\", \"features.2.2.block.3.1.bias\", \"features.2.2.block.3.1.running_mean\", \"features.2.2.block.3.1.running_var\", \"features.2.2.block.3.1.num_batches_tracked\", \"features.3.0.block.0.0.weight\", \"features.3.0.block.0.1.weight\", \"features.3.0.block.0.1.bias\", \"features.3.0.block.0.1.running_mean\", \"features.3.0.block.0.1.running_var\", \"features.3.0.block.0.1.num_batches_tracked\", \"features.3.0.block.1.0.weight\", \"features.3.0.block.1.1.weight\", \"features.3.0.block.1.1.bias\", \"features.3.0.block.1.1.running_mean\", \"features.3.0.block.1.1.running_var\", \"features.3.0.block.1.1.num_batches_tracked\", \"features.3.0.block.2.fc1.weight\", \"features.3.0.block.2.fc1.bias\", \"features.3.0.block.2.fc2.weight\", \"features.3.0.block.2.fc2.bias\", \"features.3.0.block.3.0.weight\", \"features.3.0.block.3.1.weight\", \"features.3.0.block.3.1.bias\", \"features.3.0.block.3.1.running_mean\", \"features.3.0.block.3.1.running_var\", \"features.3.0.block.3.1.num_batches_tracked\", \"features.3.1.block.0.0.weight\", \"features.3.1.block.0.1.weight\", \"features.3.1.block.0.1.bias\", \"features.3.1.block.0.1.running_mean\", \"features.3.1.block.0.1.running_var\", \"features.3.1.block.0.1.num_batches_tracked\", \"features.3.1.block.1.0.weight\", \"features.3.1.block.1.1.weight\", \"features.3.1.block.1.1.bias\", \"features.3.1.block.1.1.running_mean\", \"features.3.1.block.1.1.running_var\", \"features.3.1.block.1.1.num_batches_tracked\", \"features.3.1.block.2.fc1.weight\", \"features.3.1.block.2.fc1.bias\", \"features.3.1.block.2.fc2.weight\", \"features.3.1.block.2.fc2.bias\", \"features.3.1.block.3.0.weight\", \"features.3.1.block.3.1.weight\", \"features.3.1.block.3.1.bias\", \"features.3.1.block.3.1.running_mean\", \"features.3.1.block.3.1.running_var\", \"features.3.1.block.3.1.num_batches_tracked\", \"features.3.2.block.0.0.weight\", \"features.3.2.block.0.1.weight\", \"features.3.2.block.0.1.bias\", \"features.3.2.block.0.1.running_mean\", \"features.3.2.block.0.1.running_var\", \"features.3.2.block.0.1.num_batches_tracked\", \"features.3.2.block.1.0.weight\", \"features.3.2.block.1.1.weight\", \"features.3.2.block.1.1.bias\", \"features.3.2.block.1.1.running_mean\", \"features.3.2.block.1.1.running_var\", \"features.3.2.block.1.1.num_batches_tracked\", \"features.3.2.block.2.fc1.weight\", \"features.3.2.block.2.fc1.bias\", \"features.3.2.block.2.fc2.weight\", \"features.3.2.block.2.fc2.bias\", \"features.3.2.block.3.0.weight\", \"features.3.2.block.3.1.weight\", \"features.3.2.block.3.1.bias\", \"features.3.2.block.3.1.running_mean\", \"features.3.2.block.3.1.running_var\", \"features.3.2.block.3.1.num_batches_tracked\", \"features.4.0.block.0.0.weight\", \"features.4.0.block.0.1.weight\", \"features.4.0.block.0.1.bias\", \"features.4.0.block.0.1.running_mean\", \"features.4.0.block.0.1.running_var\", \"features.4.0.block.0.1.num_batches_tracked\", \"features.4.0.block.1.0.weight\", \"features.4.0.block.1.1.weight\", \"features.4.0.block.1.1.bias\", \"features.4.0.block.1.1.running_mean\", \"features.4.0.block.1.1.running_var\", \"features.4.0.block.1.1.num_batches_tracked\", \"features.4.0.block.2.fc1.weight\", \"features.4.0.block.2.fc1.bias\", \"features.4.0.block.2.fc2.weight\", \"features.4.0.block.2.fc2.bias\", \"features.4.0.block.3.0.weight\", \"features.4.0.block.3.1.weight\", \"features.4.0.block.3.1.bias\", \"features.4.0.block.3.1.running_mean\", \"features.4.0.block.3.1.running_var\", \"features.4.0.block.3.1.num_batches_tracked\", \"features.4.1.block.0.0.weight\", \"features.4.1.block.0.1.weight\", \"features.4.1.block.0.1.bias\", \"features.4.1.block.0.1.running_mean\", \"features.4.1.block.0.1.running_var\", \"features.4.1.block.0.1.num_batches_tracked\", \"features.4.1.block.1.0.weight\", \"features.4.1.block.1.1.weight\", \"features.4.1.block.1.1.bias\", \"features.4.1.block.1.1.running_mean\", \"features.4.1.block.1.1.running_var\", \"features.4.1.block.1.1.num_batches_tracked\", \"features.4.1.block.2.fc1.weight\", \"features.4.1.block.2.fc1.bias\", \"features.4.1.block.2.fc2.weight\", \"features.4.1.block.2.fc2.bias\", \"features.4.1.block.3.0.weight\", \"features.4.1.block.3.1.weight\", \"features.4.1.block.3.1.bias\", \"features.4.1.block.3.1.running_mean\", \"features.4.1.block.3.1.running_var\", \"features.4.1.block.3.1.num_batches_tracked\", \"features.4.2.block.0.0.weight\", \"features.4.2.block.0.1.weight\", \"features.4.2.block.0.1.bias\", \"features.4.2.block.0.1.running_mean\", \"features.4.2.block.0.1.running_var\", \"features.4.2.block.0.1.num_batches_tracked\", \"features.4.2.block.1.0.weight\", \"features.4.2.block.1.1.weight\", \"features.4.2.block.1.1.bias\", \"features.4.2.block.1.1.running_mean\", \"features.4.2.block.1.1.running_var\", \"features.4.2.block.1.1.num_batches_tracked\", \"features.4.2.block.2.fc1.weight\", \"features.4.2.block.2.fc1.bias\", \"features.4.2.block.2.fc2.weight\", \"features.4.2.block.2.fc2.bias\", \"features.4.2.block.3.0.weight\", \"features.4.2.block.3.1.weight\", \"features.4.2.block.3.1.bias\", \"features.4.2.block.3.1.running_mean\", \"features.4.2.block.3.1.running_var\", \"features.4.2.block.3.1.num_batches_tracked\", \"features.4.3.block.0.0.weight\", \"features.4.3.block.0.1.weight\", \"features.4.3.block.0.1.bias\", \"features.4.3.block.0.1.running_mean\", \"features.4.3.block.0.1.running_var\", \"features.4.3.block.0.1.num_batches_tracked\", \"features.4.3.block.1.0.weight\", \"features.4.3.block.1.1.weight\", \"features.4.3.block.1.1.bias\", \"features.4.3.block.1.1.running_mean\", \"features.4.3.block.1.1.running_var\", \"features.4.3.block.1.1.num_batches_tracked\", \"features.4.3.block.2.fc1.weight\", \"features.4.3.block.2.fc1.bias\", \"features.4.3.block.2.fc2.weight\", \"features.4.3.block.2.fc2.bias\", \"features.4.3.block.3.0.weight\", \"features.4.3.block.3.1.weight\", \"features.4.3.block.3.1.bias\", \"features.4.3.block.3.1.running_mean\", \"features.4.3.block.3.1.running_var\", \"features.4.3.block.3.1.num_batches_tracked\", \"features.5.0.block.0.0.weight\", \"features.5.0.block.0.1.weight\", \"features.5.0.block.0.1.bias\", \"features.5.0.block.0.1.running_mean\", \"features.5.0.block.0.1.running_var\", \"features.5.0.block.0.1.num_batches_tracked\", \"features.5.0.block.1.0.weight\", \"features.5.0.block.1.1.weight\", \"features.5.0.block.1.1.bias\", \"features.5.0.block.1.1.running_mean\", \"features.5.0.block.1.1.running_var\", \"features.5.0.block.1.1.num_batches_tracked\", \"features.5.0.block.2.fc1.weight\", \"features.5.0.block.2.fc1.bias\", \"features.5.0.block.2.fc2.weight\", \"features.5.0.block.2.fc2.bias\", \"features.5.0.block.3.0.weight\", \"features.5.0.block.3.1.weight\", \"features.5.0.block.3.1.bias\", \"features.5.0.block.3.1.running_mean\", \"features.5.0.block.3.1.running_var\", \"features.5.0.block.3.1.num_batches_tracked\", \"features.5.1.block.0.0.weight\", \"features.5.1.block.0.1.weight\", \"features.5.1.block.0.1.bias\", \"features.5.1.block.0.1.running_mean\", \"features.5.1.block.0.1.running_var\", \"features.5.1.block.0.1.num_batches_tracked\", \"features.5.1.block.1.0.weight\", \"features.5.1.block.1.1.weight\", \"features.5.1.block.1.1.bias\", \"features.5.1.block.1.1.running_mean\", \"features.5.1.block.1.1.running_var\", \"features.5.1.block.1.1.num_batches_tracked\", \"features.5.1.block.2.fc1.weight\", \"features.5.1.block.2.fc1.bias\", \"features.5.1.block.2.fc2.weight\", \"features.5.1.block.2.fc2.bias\", \"features.5.1.block.3.0.weight\", \"features.5.1.block.3.1.weight\", \"features.5.1.block.3.1.bias\", \"features.5.1.block.3.1.running_mean\", \"features.5.1.block.3.1.running_var\", \"features.5.1.block.3.1.num_batches_tracked\", \"features.5.2.block.0.0.weight\", \"features.5.2.block.0.1.weight\", \"features.5.2.block.0.1.bias\", \"features.5.2.block.0.1.running_mean\", \"features.5.2.block.0.1.running_var\", \"features.5.2.block.0.1.num_batches_tracked\", \"features.5.2.block.1.0.weight\", \"features.5.2.block.1.1.weight\", \"features.5.2.block.1.1.bias\", \"features.5.2.block.1.1.running_mean\", \"features.5.2.block.1.1.running_var\", \"features.5.2.block.1.1.num_batches_tracked\", \"features.5.2.block.2.fc1.weight\", \"features.5.2.block.2.fc1.bias\", \"features.5.2.block.2.fc2.weight\", \"features.5.2.block.2.fc2.bias\", \"features.5.2.block.3.0.weight\", \"features.5.2.block.3.1.weight\", \"features.5.2.block.3.1.bias\", \"features.5.2.block.3.1.running_mean\", \"features.5.2.block.3.1.running_var\", \"features.5.2.block.3.1.num_batches_tracked\", \"features.5.3.block.0.0.weight\", \"features.5.3.block.0.1.weight\", \"features.5.3.block.0.1.bias\", \"features.5.3.block.0.1.running_mean\", \"features.5.3.block.0.1.running_var\", \"features.5.3.block.0.1.num_batches_tracked\", \"features.5.3.block.1.0.weight\", \"features.5.3.block.1.1.weight\", \"features.5.3.block.1.1.bias\", \"features.5.3.block.1.1.running_mean\", \"features.5.3.block.1.1.running_var\", \"features.5.3.block.1.1.num_batches_tracked\", \"features.5.3.block.2.fc1.weight\", \"features.5.3.block.2.fc1.bias\", \"features.5.3.block.2.fc2.weight\", \"features.5.3.block.2.fc2.bias\", \"features.5.3.block.3.0.weight\", \"features.5.3.block.3.1.weight\", \"features.5.3.block.3.1.bias\", \"features.5.3.block.3.1.running_mean\", \"features.5.3.block.3.1.running_var\", \"features.5.3.block.3.1.num_batches_tracked\", \"features.6.0.block.0.0.weight\", \"features.6.0.block.0.1.weight\", \"features.6.0.block.0.1.bias\", \"features.6.0.block.0.1.running_mean\", \"features.6.0.block.0.1.running_var\", \"features.6.0.block.0.1.num_batches_tracked\", \"features.6.0.block.1.0.weight\", \"features.6.0.block.1.1.weight\", \"features.6.0.block.1.1.bias\", \"features.6.0.block.1.1.running_mean\", \"features.6.0.block.1.1.running_var\", \"features.6.0.block.1.1.num_batches_tracked\", \"features.6.0.block.2.fc1.weight\", \"features.6.0.block.2.fc1.bias\", \"features.6.0.block.2.fc2.weight\", \"features.6.0.block.2.fc2.bias\", \"features.6.0.block.3.0.weight\", \"features.6.0.block.3.1.weight\", \"features.6.0.block.3.1.bias\", \"features.6.0.block.3.1.running_mean\", \"features.6.0.block.3.1.running_var\", \"features.6.0.block.3.1.num_batches_tracked\", \"features.6.1.block.0.0.weight\", \"features.6.1.block.0.1.weight\", \"features.6.1.block.0.1.bias\", \"features.6.1.block.0.1.running_mean\", \"features.6.1.block.0.1.running_var\", \"features.6.1.block.0.1.num_batches_tracked\", \"features.6.1.block.1.0.weight\", \"features.6.1.block.1.1.weight\", \"features.6.1.block.1.1.bias\", \"features.6.1.block.1.1.running_mean\", \"features.6.1.block.1.1.running_var\", \"features.6.1.block.1.1.num_batches_tracked\", \"features.6.1.block.2.fc1.weight\", \"features.6.1.block.2.fc1.bias\", \"features.6.1.block.2.fc2.weight\", \"features.6.1.block.2.fc2.bias\", \"features.6.1.block.3.0.weight\", \"features.6.1.block.3.1.weight\", \"features.6.1.block.3.1.bias\", \"features.6.1.block.3.1.running_mean\", \"features.6.1.block.3.1.running_var\", \"features.6.1.block.3.1.num_batches_tracked\", \"features.6.2.block.0.0.weight\", \"features.6.2.block.0.1.weight\", \"features.6.2.block.0.1.bias\", \"features.6.2.block.0.1.running_mean\", \"features.6.2.block.0.1.running_var\", \"features.6.2.block.0.1.num_batches_tracked\", \"features.6.2.block.1.0.weight\", \"features.6.2.block.1.1.weight\", \"features.6.2.block.1.1.bias\", \"features.6.2.block.1.1.running_mean\", \"features.6.2.block.1.1.running_var\", \"features.6.2.block.1.1.num_batches_tracked\", \"features.6.2.block.2.fc1.weight\", \"features.6.2.block.2.fc1.bias\", \"features.6.2.block.2.fc2.weight\", \"features.6.2.block.2.fc2.bias\", \"features.6.2.block.3.0.weight\", \"features.6.2.block.3.1.weight\", \"features.6.2.block.3.1.bias\", \"features.6.2.block.3.1.running_mean\", \"features.6.2.block.3.1.running_var\", \"features.6.2.block.3.1.num_batches_tracked\", \"features.6.3.block.0.0.weight\", \"features.6.3.block.0.1.weight\", \"features.6.3.block.0.1.bias\", \"features.6.3.block.0.1.running_mean\", \"features.6.3.block.0.1.running_var\", \"features.6.3.block.0.1.num_batches_tracked\", \"features.6.3.block.1.0.weight\", \"features.6.3.block.1.1.weight\", \"features.6.3.block.1.1.bias\", \"features.6.3.block.1.1.running_mean\", \"features.6.3.block.1.1.running_var\", \"features.6.3.block.1.1.num_batches_tracked\", \"features.6.3.block.2.fc1.weight\", \"features.6.3.block.2.fc1.bias\", \"features.6.3.block.2.fc2.weight\", \"features.6.3.block.2.fc2.bias\", \"features.6.3.block.3.0.weight\", \"features.6.3.block.3.1.weight\", \"features.6.3.block.3.1.bias\", \"features.6.3.block.3.1.running_mean\", \"features.6.3.block.3.1.running_var\", \"features.6.3.block.3.1.num_batches_tracked\", \"features.6.4.block.0.0.weight\", \"features.6.4.block.0.1.weight\", \"features.6.4.block.0.1.bias\", \"features.6.4.block.0.1.running_mean\", \"features.6.4.block.0.1.running_var\", \"features.6.4.block.0.1.num_batches_tracked\", \"features.6.4.block.1.0.weight\", \"features.6.4.block.1.1.weight\", \"features.6.4.block.1.1.bias\", \"features.6.4.block.1.1.running_mean\", \"features.6.4.block.1.1.running_var\", \"features.6.4.block.1.1.num_batches_tracked\", \"features.6.4.block.2.fc1.weight\", \"features.6.4.block.2.fc1.bias\", \"features.6.4.block.2.fc2.weight\", \"features.6.4.block.2.fc2.bias\", \"features.6.4.block.3.0.weight\", \"features.6.4.block.3.1.weight\", \"features.6.4.block.3.1.bias\", \"features.6.4.block.3.1.running_mean\", \"features.6.4.block.3.1.running_var\", \"features.6.4.block.3.1.num_batches_tracked\", \"features.7.0.block.0.0.weight\", \"features.7.0.block.0.1.weight\", \"features.7.0.block.0.1.bias\", \"features.7.0.block.0.1.running_mean\", \"features.7.0.block.0.1.running_var\", \"features.7.0.block.0.1.num_batches_tracked\", \"features.7.0.block.1.0.weight\", \"features.7.0.block.1.1.weight\", \"features.7.0.block.1.1.bias\", \"features.7.0.block.1.1.running_mean\", \"features.7.0.block.1.1.running_var\", \"features.7.0.block.1.1.num_batches_tracked\", \"features.7.0.block.2.fc1.weight\", \"features.7.0.block.2.fc1.bias\", \"features.7.0.block.2.fc2.weight\", \"features.7.0.block.2.fc2.bias\", \"features.7.0.block.3.0.weight\", \"features.7.0.block.3.1.weight\", \"features.7.0.block.3.1.bias\", \"features.7.0.block.3.1.running_mean\", \"features.7.0.block.3.1.running_var\", \"features.7.0.block.3.1.num_batches_tracked\", \"features.7.1.block.0.0.weight\", \"features.7.1.block.0.1.weight\", \"features.7.1.block.0.1.bias\", \"features.7.1.block.0.1.running_mean\", \"features.7.1.block.0.1.running_var\", \"features.7.1.block.0.1.num_batches_tracked\", \"features.7.1.block.1.0.weight\", \"features.7.1.block.1.1.weight\", \"features.7.1.block.1.1.bias\", \"features.7.1.block.1.1.running_mean\", \"features.7.1.block.1.1.running_var\", \"features.7.1.block.1.1.num_batches_tracked\", \"features.7.1.block.2.fc1.weight\", \"features.7.1.block.2.fc1.bias\", \"features.7.1.block.2.fc2.weight\", \"features.7.1.block.2.fc2.bias\", \"features.7.1.block.3.0.weight\", \"features.7.1.block.3.1.weight\", \"features.7.1.block.3.1.bias\", \"features.7.1.block.3.1.running_mean\", \"features.7.1.block.3.1.running_var\", \"features.7.1.block.3.1.num_batches_tracked\", \"features.8.0.weight\", \"features.8.1.weight\", \"features.8.1.bias\", \"features.8.1.running_mean\", \"features.8.1.running_var\", \"features.8.1.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "model2 = FusionModel(meta_input_dim=19, num_classes=7)\n",
    "model2 = train_cnn_first(model2, train_loader, val_loader, epochs=20)\n",
    "model2 = train_fusion_model(model2, train_loader, val_loader, epochs=20)\n",
    "results = test_model(model2, test_loader, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fb627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "\n",
    "# Initialize model\n",
    "model_save = FusionModel(meta_input_dim=19, num_classes=7)\n",
    "\n",
    "# Load only the model weights\n",
    "model_save.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7a08377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fusion_with_checkpoint(\n",
    "    checkpoint_path='best_cnn_model.pth',\n",
    "    meta_input_dim=19,\n",
    "    num_classes=7,\n",
    "    train_loader=None,\n",
    "    val_loader=None,\n",
    "    fusion_epochs=20,\n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"\n",
    "    Train fusion model using pre-trained CNN weights\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to saved CNN checkpoint\n",
    "        meta_input_dim: Dimension of metadata features\n",
    "        num_classes: Number of output classes\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        fusion_epochs: Number of epochs for fusion training\n",
    "        device: Device to use ('cuda' or 'cpu')\n",
    "    \"\"\"\n",
    "    # 1. Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "    print(f\"Previous val accuracy: {checkpoint.get('val_acc', 0):.4f}\")\n",
    "    \n",
    "    # 2. Initialize model\n",
    "    model = FusionModel(meta_input_dim=meta_input_dim, num_classes=num_classes)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 3. Load CNN weights\n",
    "    model.image_model.load_state_dict({\n",
    "        k.replace('image_model.', ''): v \n",
    "        for k, v in checkpoint['model_state_dict'].items()\n",
    "        if k.startswith('image_model.')\n",
    "    })\n",
    "    \n",
    "    # 4. Freeze CNN and prepare for fusion training\n",
    "    model.freeze_cnn()\n",
    "    model.unfreeze_classifier()\n",
    "    \n",
    "    # 5. Set up optimizer (only for unfrozen layers)\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.meta_net.parameters()},\n",
    "        {'params': model.classifier.parameters()}\n",
    "    ], lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # 6. Fusion training loop\n",
    "    for epoch in range(fusion_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for images, metadata, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{fusion_epochs}\"):\n",
    "            images = images.to(device)\n",
    "            metadata = metadata.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, metadata)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = accuracy_score(all_targets, all_preds)\n",
    "        train_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc, val_f1 = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{fusion_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, 'best_fusion_model.pth')\n",
    "            print(\"Saved new best fusion model\")\n",
    "    \n",
    "    print(\"\\nFusion training complete!\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Helper validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, metadata, targets in val_loader:\n",
    "            images = images.to(device)\n",
    "            metadata = metadata.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(images, metadata)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    val_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    return val_loss, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "052d52fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 9\n",
      "Previous val accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Train Loss: 0.2141 | Acc: 0.9612 | F1: 0.9612\n",
      "Val Loss: 0.5704 | Acc: 0.8419 | F1: 0.8428\n",
      "Saved new best fusion model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 109/109 [00:43<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20\n",
      "Train Loss: 0.0942 | Acc: 0.9743 | F1: 0.9742\n",
      "Val Loss: 0.6112 | Acc: 0.8486 | F1: 0.8493\n",
      "Saved new best fusion model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20\n",
      "Train Loss: 0.0746 | Acc: 0.9769 | F1: 0.9768\n",
      "Val Loss: 0.6722 | Acc: 0.8426 | F1: 0.8415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 109/109 [00:43<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20\n",
      "Train Loss: 0.0756 | Acc: 0.9770 | F1: 0.9770\n",
      "Val Loss: 0.6904 | Acc: 0.8332 | F1: 0.8371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20\n",
      "Train Loss: 0.0784 | Acc: 0.9760 | F1: 0.9760\n",
      "Val Loss: 0.6914 | Acc: 0.8399 | F1: 0.8407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 109/109 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20\n",
      "Train Loss: 0.0698 | Acc: 0.9782 | F1: 0.9781\n",
      "Val Loss: 0.6645 | Acc: 0.8439 | F1: 0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 109/109 [00:44<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20\n",
      "Train Loss: 0.0686 | Acc: 0.9769 | F1: 0.9769\n",
      "Val Loss: 0.7350 | Acc: 0.8346 | F1: 0.8343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 109/109 [00:43<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/20\n",
      "Train Loss: 0.0636 | Acc: 0.9794 | F1: 0.9794\n",
      "Val Loss: 0.7336 | Acc: 0.8392 | F1: 0.8411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 109/109 [00:44<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0686 | Acc: 0.9753 | F1: 0.9752\n",
      "Val Loss: 0.7076 | Acc: 0.8433 | F1: 0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 109/109 [00:43<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/20\n",
      "Train Loss: 0.0608 | Acc: 0.9796 | F1: 0.9796\n",
      "Val Loss: 0.7255 | Acc: 0.8406 | F1: 0.8390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 109/109 [00:43<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0668 | Acc: 0.9776 | F1: 0.9776\n",
      "Val Loss: 0.7355 | Acc: 0.8446 | F1: 0.8462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 109/109 [00:44<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/20\n",
      "Train Loss: 0.0610 | Acc: 0.9796 | F1: 0.9796\n",
      "Val Loss: 0.7619 | Acc: 0.8426 | F1: 0.8408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 109/109 [00:44<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/20\n",
      "Train Loss: 0.0653 | Acc: 0.9794 | F1: 0.9794\n",
      "Val Loss: 0.7455 | Acc: 0.8439 | F1: 0.8443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 109/109 [00:43<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/20\n",
      "Train Loss: 0.0690 | Acc: 0.9786 | F1: 0.9786\n",
      "Val Loss: 0.7638 | Acc: 0.8359 | F1: 0.8368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/20\n",
      "Train Loss: 0.0524 | Acc: 0.9826 | F1: 0.9826\n",
      "Val Loss: 0.7560 | Acc: 0.8386 | F1: 0.8395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 109/109 [00:43<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/20\n",
      "Train Loss: 0.0530 | Acc: 0.9819 | F1: 0.9819\n",
      "Val Loss: 0.7575 | Acc: 0.8419 | F1: 0.8435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 109/109 [00:43<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/20\n",
      "Train Loss: 0.0497 | Acc: 0.9832 | F1: 0.9832\n",
      "Val Loss: 0.7801 | Acc: 0.8453 | F1: 0.8448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 109/109 [00:42<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/20\n",
      "Train Loss: 0.0680 | Acc: 0.9790 | F1: 0.9790\n",
      "Val Loss: 0.7731 | Acc: 0.8426 | F1: 0.8416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 109/109 [00:44<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/20\n",
      "Train Loss: 0.0766 | Acc: 0.9743 | F1: 0.9742\n",
      "Val Loss: 0.7711 | Acc: 0.8366 | F1: 0.8394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 109/109 [00:44<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/20\n",
      "Train Loss: 0.0575 | Acc: 0.9825 | F1: 0.9825\n",
      "Val Loss: 0.8222 | Acc: 0.8386 | F1: 0.8409\n",
      "\n",
      "Fusion training complete!\n",
      "Best validation accuracy: 0.8486\n"
     ]
    }
   ],
   "source": [
    "fusion_model = train_fusion_with_checkpoint(\n",
    "    checkpoint_path='best_model.pth',\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    fusion_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0e1e961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 24/24 [00:08<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PER-CLASS METRICS ===\n",
      "\n",
      "Class: nv\n",
      "Accuracy: 0.6648\n",
      "Precision: 0.6223\n",
      "Recall: 0.6648\n",
      "F1: 0.6429\n",
      "AUC: 0.9327\n",
      "\n",
      "Class: mel\n",
      "Accuracy: 0.9070\n",
      "Precision: 0.9135\n",
      "Recall: 0.9070\n",
      "F1: 0.9102\n",
      "AUC: 0.9403\n",
      "\n",
      "Class: bkl\n",
      "Accuracy: 0.5556\n",
      "Precision: 0.4545\n",
      "Recall: 0.5556\n",
      "F1: 0.5000\n",
      "AUC: 0.9632\n",
      "\n",
      "Class: bcc\n",
      "Accuracy: 0.5731\n",
      "Precision: 0.5833\n",
      "Recall: 0.5731\n",
      "F1: 0.5782\n",
      "AUC: 0.8956\n",
      "\n",
      "Class: akiec\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8333\n",
      "Recall: 0.7692\n",
      "F1: 0.8000\n",
      "AUC: 0.9926\n",
      "\n",
      "Class: vasc\n",
      "Accuracy: 0.6842\n",
      "Precision: 0.7831\n",
      "Recall: 0.6842\n",
      "F1: 0.7303\n",
      "AUC: 0.9808\n",
      "\n",
      "Class: df\n",
      "Accuracy: 0.6038\n",
      "Precision: 0.5079\n",
      "Recall: 0.6038\n",
      "F1: 0.5517\n",
      "AUC: 0.9274\n",
      "\n",
      "=== OVERALL METRICS ===\n",
      "Accuracy: 0.8117\n",
      "\n",
      "Macro Averages:\n",
      "Precision: 0.6712\n",
      "Recall: 0.6797\n",
      "F1: 0.6733\n",
      "AUC: 0.9475\n",
      "\n",
      "Weighted Averages:\n",
      "Precision: 0.8155\n",
      "Recall: 0.8117\n",
      "F1: 0.8131\n",
      "AUC: 0.9375\n"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint2 = torch.load('best_fusion_model.pth')\n",
    "\n",
    "# Initialize model\n",
    "model_save2 = FusionModel(meta_input_dim=19, num_classes=7)\n",
    "\n",
    "# Load only the model weights\n",
    "model_save2.load_state_dict(checkpoint2['model_state_dict'])\n",
    "results = test_model(model_save2, test_loader, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
